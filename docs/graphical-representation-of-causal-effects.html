<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6. Graphical representation of causal effects | Causal inference</title>
  <meta name="description" content="6. Graphical representation of causal effects | Causal inference" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="6. Graphical representation of causal effects | Causal inference" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6. Graphical representation of causal effects | Causal inference" />
  
  
  

<meta name="author" content="Duzhe Wang" />


<meta name="date" content="2021-04-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="interaction.html"/>
<link rel="next" href="why-model.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R code for causal inference book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i>5. Interaction</a><ul>
<li class="chapter" data-level="" data-path="interaction.html"><a href="interaction.html#interaction-requires-a-joint-intervention"><i class="fa fa-check"></i>5.1 Interaction requires a joint intervention</a></li>
<li class="chapter" data-level="" data-path="interaction.html"><a href="interaction.html#identifying-interaction"><i class="fa fa-check"></i>5.2 Identifying interaction</a></li>
<li class="chapter" data-level="" data-path="interaction.html"><a href="interaction.html#counterfactual-response-types-and-interaction"><i class="fa fa-check"></i>5.3 Counterfactual response types and interaction</a></li>
<li class="chapter" data-level="" data-path="interaction.html"><a href="interaction.html#sufficient-causes"><i class="fa fa-check"></i>5.4 Sufficient causes</a></li>
<li class="chapter" data-level="" data-path="interaction.html"><a href="interaction.html#sufficient-cause-interaction"><i class="fa fa-check"></i>5.5 Sufficient cause interaction</a></li>
<li class="chapter" data-level="" data-path="interaction.html"><a href="interaction.html#counterfactuals-or-sufficient-component-causes"><i class="fa fa-check"></i>5.6 Counterfactuals or sufficient-component causes?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html"><i class="fa fa-check"></i>6. Graphical representation of causal effects</a><ul>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#causal-diagrams"><i class="fa fa-check"></i>6.1 Causal diagrams</a><ul>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#causal-directed-acyclic-graphs"><i class="fa fa-check"></i>Causal directed acyclic graphs</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#examples"><i class="fa fa-check"></i>Examples</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#causal-diagrams-and-marginal-independence"><i class="fa fa-check"></i>6.2 Causal diagrams and marginal independence</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#causal-diagrams-and-conditional-independence"><i class="fa fa-check"></i>6.3 Causal diagrams and conditional independence</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#appendix-a-uncorrelated-vs.independent"><i class="fa fa-check"></i>Appendix A: uncorrelated vs. independent</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#appendix-b-the-flow-of-association-and-causation-in-graphs"><i class="fa fa-check"></i>Appendix B: The flow of association and causation in graphs</a><ul>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#graph-terminology"><i class="fa fa-check"></i>Graph terminology</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#bayesian-networks"><i class="fa fa-check"></i>Bayesian networks</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#causal-graphs"><i class="fa fa-check"></i>Causal graphs</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#two-node-graphs-and-graphical-building-blocks"><i class="fa fa-check"></i>Two-node graphs and graphical building blocks</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#chains-and-forks"><i class="fa fa-check"></i>Chains and forks</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#colliders-and-their-descendants"><i class="fa fa-check"></i>Colliders and their descendants</a></li>
<li class="chapter" data-level="" data-path="graphical-representation-of-causal-effects.html"><a href="graphical-representation-of-causal-effects.html#d-separation"><i class="fa fa-check"></i>d-separation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="why-model.html"><a href="why-model.html"><i class="fa fa-check"></i>11. Why model?</a><ul>
<li class="chapter" data-level="" data-path="why-model.html"><a href="why-model.html#some-concepts-and-points"><i class="fa fa-check"></i>Some concepts and points</a></li>
<li class="chapter" data-level="" data-path="why-model.html"><a href="why-model.html#program-11.1"><i class="fa fa-check"></i>Program 11.1</a></li>
<li class="chapter" data-level="" data-path="why-model.html"><a href="why-model.html#program-11.2"><i class="fa fa-check"></i>Program 11.2</a></li>
<li class="chapter" data-level="" data-path="why-model.html"><a href="why-model.html#program-11.3"><i class="fa fa-check"></i>Program 11.3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html"><i class="fa fa-check"></i>12. IP weighting and marginal structural models</a><ul>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#part-1-summary-of-the-chapter"><i class="fa fa-check"></i>Part 1: Summary of the chapter</a><ul>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#definitions"><i class="fa fa-check"></i>Definitions</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#equivalence-of-ip-weighting-and-standardization"><i class="fa fa-check"></i>Equivalence of IP weighting and standardization</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#equivalence-of-potential-outcome-mean-standardized-mean-and-ip-weighted-mean"><i class="fa fa-check"></i>Equivalence of potential outcome mean, standardized mean and IP weighted mean</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#what-does-ip-weighting-mean"><i class="fa fa-check"></i>What does IP weighting mean?</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#an-example"><i class="fa fa-check"></i>An example</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#horvitz-thompson-estimator-and-hajek-estimator"><i class="fa fa-check"></i>Horvitz-Thompson estimator and Hajek estimator</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#stablized-ip-weights"><i class="fa fa-check"></i>Stablized IP weights</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#marginal-structural-models"><i class="fa fa-check"></i>Marginal structural models</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#effect-modification-and-marginal-structural-models"><i class="fa fa-check"></i>Effect modification and marginal structural models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#part-2-real-data-analysis"><i class="fa fa-check"></i>Part 2: Real data analysis</a><ul>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#background"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#input-dataset"><i class="fa fa-check"></i>Input dataset</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#ignore-subjects-with-missing-values-for-weight-in-1982"><i class="fa fa-check"></i>Ignore subjects with missing values for weight in 1982</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#compare-the-treatment-group-and-the-control-group"><i class="fa fa-check"></i>Compare the treatment group and the control group</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#estimating-ip-weights-via-modeling"><i class="fa fa-check"></i>Estimating IP weights via modeling</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#stablized-ip-weights-1"><i class="fa fa-check"></i>Stablized IP weights</a></li>
<li class="chapter" data-level="" data-path="ip-weighting-and-marginal-structural-models.html"><a href="ip-weighting-and-marginal-structural-models.html#marginal-structural-models-1"><i class="fa fa-check"></i>Marginal structural models</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="graphical-representation-of-causal-effects" class="section level1 unnumbered">
<h1>6. Graphical representation of causal effects</h1>
<p>The use of graphs in causal inference problems makes it easier to follow a sensible advice: draw your assumptions before your conclusions.</p>
<div id="causal-diagrams" class="section level2 unnumbered">
<h2>6.1 Causal diagrams</h2>
<!--<img src="images/fig6-1.png">

- The presence of an arrow pointing from a particular variable $V$ to another variable $W$ indicates that we know there is a direct causal effect for at least one individual. 

- A standard causal diagram does not distinguish whether an arrow represents a harmful effect or a protective effect. 

-->
<div id="causal-directed-acyclic-graphs" class="section level3 unnumbered">
<h3>Causal directed acyclic graphs</h3>
<ul>
<li><p>We define a DAG <span class="math inline">\(G\)</span> to be a graph whose nodes are random variables <span class="math inline">\(V=(V_1,..., V_M)\)</span> with directed edges and no directed cycles. We use <span class="math inline">\(PA_m\)</span> to denote the parents of <span class="math inline">\(V_m\)</span>, i.e., the set of nodes from which there is a direct arrow into <span class="math inline">\(V_m\)</span>. The variable <span class="math inline">\(V_m\)</span> is a descendant of <span class="math inline">\(V_j\)</span> if there is a sequence of nodes connected by edges between <span class="math inline">\(V_j\)</span> and <span class="math inline">\(V_m\)</span> such that, following the direction indicated by the arrows, one can reach <span class="math inline">\(V_m\)</span> by starting at <span class="math inline">\(V_j\)</span>. We adopt the ordering convention that if <span class="math inline">\(m&gt;j\)</span>, <span class="math inline">\(V_m\)</span> is not an ancestor of <span class="math inline">\(V_j\)</span>. We define the distribution of <span class="math inline">\(V\)</span> to be Markov with respect to a DAG <span class="math inline">\(G\)</span> if for each <span class="math inline">\(j\)</span>, <span class="math inline">\(V_j\)</span> is independent of its non-descendants conditional on its parents.</p></li>
<li><p>A causal DAG is a DAG in which 1) the lack of an arrow from node <span class="math inline">\(V_j\)</span> to <span class="math inline">\(V_m\)</span> can be interpreted as the absence of a direct causal effect of <span class="math inline">\(V_j\)</span> on <span class="math inline">\(V_m\)</span> relative to the other variables on the graph, <strong>2) all common causes, even if unmeasured, of any pair of variables on the graph are themselves on the graph</strong>, and 3) any variable is a cause of its descendants.</p></li>
<li><p>Causal Markov assumption: conditional on its direct causes, a variable <span class="math inline">\(V_j\)</span> is independent of any variable for which it is not a cause. That is, conditional on its parents, <span class="math inline">\(V_j\)</span> is independent of its non-descendants. Mathematically, it’s equivalent to the statement that the density <span class="math inline">\(f(V)\)</span> of the variables <span class="math inline">\(V\)</span> in DAG <span class="math inline">\(G\)</span> satisfies the Markov factorization
<span class="math display">\[ f(v)=\prod_{i=1}^{M}f(v_j|pa_{j}). \]</span></p></li>
</ul>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We want to show the equivalence. We have
<span class="math display">\[f(v)=f(v_1)\prod_{i=2}^{M}f(v_j|v_1,..., v_{j-1} ).\]</span>
Based on the Markov assumption and the ordering convention, we have
<span class="math display">\[f(v_j|v_1,..., v_{j-1})=f(v_j|pa_{j}).\]</span>
Therefore, we have <span class="math inline">\(f(v)=\prod_{i=1}^{M}f(v_j|pa_{j})\)</span>.</p>
<p>Theorem 20.3 in the book All of Statistics also has an explanation.</p>
</div>

</div>
<div id="examples" class="section level3 unnumbered">
<h3>Examples</h3>
<ul>
<li>A marginally randomized experiment can be represented by the following causal DAG:
<img src="images/fig6-2.png"></li>
</ul>
<p>For example, if we know that aspirin use <span class="math inline">\(A\)</span> has a preventive causal effect on the risk of heart disease <span class="math inline">\(Y\)</span>, i.e., <span class="math inline">\(P(Y^{a=1}=1)\ne P(Y^{a=0}=1)\)</span>. The causal diagram in Figure 6.2 is the graphical translation of this knowledge for an experiment in which aspirin <span class="math inline">\(A\)</span> is randomly, and unconditionally, assigned.</p>
<ul>
<li>A conditionally randomized experiment can be represented by the following causal DAG:
<img src="images/fig6-1.png"></li>
</ul>
<p>Note this figure can also represent an observational study. Figure 6.1 represents an observational study in which we are willing to assume that the assignment of <span class="math inline">\(A\)</span> has <span class="math inline">\(L\)</span> as parent and no other causes of <span class="math inline">\(Y\)</span>. Otherwise, those causes of <span class="math inline">\(Y\)</span>, even if unmeasured, would need to be included in the diagram, as they would be common causes of <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>.</p>
<ul>
<li>Suppose we know that carrying a lighter <span class="math inline">\(A\)</span> has no causal effect on anyone’s risk of lung cancer <span class="math inline">\(Y\)</span>, i.e., <span class="math inline">\(P(Y^{a=1}=1)=P(Y^{a=0}=1)\)</span>, and that cigarette smoking <span class="math inline">\(L\)</span> hs a causal effect on both carrying a lighter <span class="math inline">\(A\)</span> and lung cancer <span class="math inline">\(Y\)</span>. The causal diagram in Figure 6.3 is the graphical translation of this knowledge.</li>
</ul>
<p><img src="images/fig6-3.png"></p>
</div>
</div>
<div id="causal-diagrams-and-marginal-independence" class="section level2 unnumbered">
<h2>6.2 Causal diagrams and marginal independence</h2>
<ul>
<li><p>Causal diagrams are a simple way to encode our subject-matter knowledge, and our assumptions, about the qualitative causal structure of a problem. Causal diagrams also encode information about potential associations between the variables in the causal network. It is precisely this simultaneous representation of association and causation that makes causal diagrams such an attractive tool.</p></li>
<li><p>When one knows that <span class="math inline">\(A\)</span> has a causal effect on <span class="math inline">\(Y\)</span>, as in Figure 6.2, then one should also generally expect <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> to be associated. This is consistenct with the fact that, in an ideal randomized experiment with unconditional exchangeability, causation <span class="math inline">\(P(Y^{a=1}=1)\ne P(Y^{a=0}=1)\)</span> implies association <span class="math inline">\(P(Y=1|A=1)\ne P(Y=1|A=0)\)</span>, and vice versa.</p></li>
<li><p>In Figure 6.3, we have <span class="math inline">\(P(Y^{a=1}=1)=P(Y^{a=0}=1)\)</span>. But <span class="math inline">\(P(Y=1|A=1)\ne P(Y=1|A=0)\)</span>. See the book for an intuitive explanation.</p></li>
<li><p>Consider the following causal DAG:</p></li>
</ul>
<p><image src="images/fig6-4.png"></p>
<p>The common effect <span class="math inline">\(L\)</span> is referred to as a collider on the path <span class="math inline">\(A\rightarrow L\leftarrow Y\)</span> because two arrowheads collide on this node. Colliders, unlike other variables, block the flow of association along the path on which they lie. Thus, <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> are independent because the only path between them, <span class="math inline">\(A\rightarrow L\leftarrow Y\)</span>, is blocked by the collider <span class="math inline">\(L\)</span>.</p>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> By Markov assumption, we have
<span class="math display">\[ f(a, y, l)=f(a)f(y)f(l|a, y).\]</span>
So
<span class="math display">\[f(a, y)=\int f(a, y, l)dl=\int f(a)f(y)f(l|a, y)dl=f(a)f(y).\]</span>
Therefore, <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> are independent.
</div>

<ul>
<li><strong>In summary, two variables are marginally associated if one causes the other, or if they share common causes. Otherwise they will be marginally independent.</strong></li>
</ul>
</div>
<div id="causal-diagrams-and-conditional-independence" class="section level2 unnumbered">
<h2>6.3 Causal diagrams and conditional independence</h2>
<ul>
<li>Consider the following causal DAG:</li>
</ul>
<p><image src="images/fig6-5.png"></p>
<p>Question: is there an association between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> within levels of (conditional on) <span class="math inline">\(B\)</span>?</p>
<p>Conclusion: Even though <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> are marginally associated, <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> are conditionally independent given <span class="math inline">\(B\)</span>. Graphically, we say that a box placed around variable <span class="math inline">\(B\)</span> blocks the flow of association through the path <span class="math inline">\(A\rightarrow B\rightarrow Y\)</span>.</p>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> By Markov assumption (conditional on its direct causes, a variable <span class="math inline">\(V_j\)</span> is independent of any variable for which it is not a cause), we have <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(A\)</span> conditional on <span class="math inline">\(B\)</span>.
</div>

<ul>
<li>Consider the following causal DAG:
<image src="images/fig6-6.png"></li>
</ul>
<p>Question: is <span class="math inline">\(A\)</span> associated with <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(L\)</span>?</p>
<p>Conclusion: <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> are conditionally independent given <span class="math inline">\(L\)</span>.</p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> By Markov assumption.</p>
</div>

<ul>
<li>Consider the following causal DAG:
<image src="images/fig6-6.png"></li>
</ul>
<p>Question: Is <span class="math inline">\(A\)</span> associated with <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(L\)</span>?</p>
<p>Conclusion:</p>
</div>
<div id="appendix-a-uncorrelated-vs.independent" class="section level2 unnumbered">
<h2>Appendix A: uncorrelated vs. independent</h2>
<ul>
<li><p>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>uncorrelated</strong> when their correlation coefficient is 0: <span class="math inline">\(\rho(X, Y)=0\)</span>.
Being uncorrelated is the same as having zero covariance. If <span class="math inline">\(\rho(X, Y)\ne 0\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are correlated.</p></li>
<li><p>Two random variables are <strong>independent</strong> when their joint probability distribution is the product of their marginal probability distributions: for all <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>,
<span class="math display">\[p_{X, Y}(x, y)=p_{X}(x)p_{Y}(y).\]</span>
Equivalently, the conditional distribution is the same as the marginal distribution:
<span class="math display">\[p_{Y|X}(y|x)=P_{Y}(y).\]</span>
If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not independent, then they are <strong>dependent</strong>.</p></li>
<li><p>If <span class="math inline">\(Y\)</span> is a non-constant function of <span class="math inline">\(X\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are always dependent. (Lemma: Let <span class="math inline">\(X\)</span> be a random variable and let <span class="math inline">\(f\)</span> be a Borel measurable function such that <span class="math inline">\(X\)</span> and <span class="math inline">\(f(X)\)</span> are independent. Then <span class="math inline">\(f(X)\)</span> is constant almost surely. That is, there is some <span class="math inline">\(a\in \mathbb{R}\)</span> such that <span class="math inline">\(P(f(X)=a)=1\)</span>.)</p></li>
</ul>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> See <a href="https://stats.stackexchange.com/questions/16321/are-the-random-variables-x-and-fx-dependent" class="uri">https://stats.stackexchange.com/questions/16321/are-the-random-variables-x-and-fx-dependent</a>.
</div>

<ul>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then they are also uncorrelated. (This also implies that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are correlated, then they are dependent.)</li>
</ul>
<p><span class="math display">\[\begin{equation*}
E(XY)=\int\int xyp_{X, Y}(x, y)dxdy
= \int\int xyp_{X}(x)p_{Y}(y)dxdy  \\ 
= \int xp_{X}(x)dx\int yp_{Y}(y)dy
=E(X)E(Y)
\end{equation*}\]</span></p>
<ul>
<li>However, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated, then they can still be dependent. For example, let <span class="math inline">\(Y=X^2\)</span>, then if <span class="math inline">\(E(X)=0\)</span> and <span class="math inline">\(E(X^3)=0\)</span>, we have <span class="math inline">\(Cov(X, Y)=E(XY)=E(X^3)=0\)</span>, so <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated, but they are dependent based on the above lemma.</li>
</ul>
</div>
<div id="appendix-b-the-flow-of-association-and-causation-in-graphs" class="section level2 unnumbered">
<h2>Appendix B: The flow of association and causation in graphs</h2>
<p>This section is mainly from Brady Neal’s Introduction to Causal Inference from a Machine Learning Perspective.</p>
<div id="graph-terminology" class="section level3 unnumbered">
<h3>Graph terminology</h3>
<ul>
<li>If two parents <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> share some child <span class="math inline">\(Z\)</span>, but there is no edge connecting <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, then <span class="math inline">\(X\rightarrow Z\leftarrow Y\)</span> is known as an <strong>immorality</strong>.</li>
</ul>
</div>
<div id="bayesian-networks" class="section level3 unnumbered">
<h3>Bayesian networks</h3>
<ul>
<li><p>Chain rule: <span class="math inline">\(P(x_1, x_2,..., x_n)=P(x_1)\prod_i P(x_i|x_{i-1},...,x_1)\)</span></p></li>
<li><p>Given a probability distribution and a corresponding DAG, we can formalize the specification of independencies with the <strong>local Markov assumption</strong>: <strong>Given its parents in the DAG, a node <span class="math inline">\(X\)</span> is independent of its non-descendants.</strong></p></li>
<li><p>A probability distribution is said to be locally Markov with respect to a DAG if they satisfy the local Markov assumption.</p></li>
<li><p><strong>Bayesian network factorization</strong>: <strong>Given a probability distribution P and a DAG G, P factorizes according to <span class="math inline">\(G\)</span> if <span class="math inline">\(P(x_1,..., x_n)=\prod_i P(x_i|pa_i)\)</span>.</strong></p></li>
<li><p>Local Markov assumption and Bayesian network factorization are equivalent.</p></li>
</ul>
<!--
\BeginKnitrBlock{proof}<div class="proof">\iffalse{} <span class="proof"><em>Proof. </em></span>  \fi{}- From local Markov assumption to Bayesian network factorization: this is straightforward. 

- From Bayesian network factorization to local Markov assumption: 


</div>\EndKnitrBlock{proof}


-->
<ul>
<li>The local Markov assumption only gives us information about the independencies in P that a DAG implies. It does not even tell us that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are adjacent in the DAGs, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are dependent. And this additional information is very commonly assumed in causal DAGs.To get the guaranteed dependence between adjacent nodes, we generally assume a slightly stronger assumption than the local Markov assumption, which is called the
<strong>minimality assumption</strong>:
<ul>
<li><ol style="list-style-type: decimal">
<li>Local Markov assumption</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>Adjacent nodes in the DAG are dependent</li>
</ol></li>
</ul></li>
<li><strong>Note the second part of minimality assumption is equivalent to the following way: if we were to remove any edges from the DAG, P would not be Markov with respect to the graph with the removed edges.</strong>
<ul>
<li>We show this equivalence by a concrete example: see the following DAG.
<p align="center">
<image src="images/minimality.png">
</p>
We first show part 2 of the minimality assumption implies the above statement. Based on the local Markov assumption, we have
<span class="math display">\[f(x_1, x_2, x_3, x_4)=f(x_1)f(x_2|x_1)f(x_3|x_1, x_2)f(x_4|x_3). \]</span>
Consider the case that we remove the edge <span class="math inline">\(X_2\rightarrow X_3\)</span>. If <span class="math inline">\(P\)</span> is Markov with respect to the graph with the removed edge, then we have
<span class="math display">\[f(x_1, x_2, x_3, x_4)=f(x_1)f(x_2|x_1)f(x_3|x_1)f(x_4|x_3).\]</span>
Therefore, we have
<span class="math display">\[f(x_3|x_1, x_2)=f(x_3|x_1),\]</span>
which implies that <span class="math inline">\(f(x_3|x_2)=f(x_3)\)</span>. Then <span class="math inline">\(X_3\)</span> is independent of <span class="math inline">\(X_2\)</span>. This contracts with part 2 of the minimality assumption. So we have <span class="math inline">\(P\)</span> is not Markov with respect to the graph with the removed edges. Next, we show the other direction. Since <span class="math inline">\(P\)</span> would not be Markov respect to the graph with the removed edges, so we have
<span class="math inline">\(f(x_3|x_1, x_2)\ne f(x_3|x_1)\)</span>, which implies that <span class="math inline">\(f(x_3|x_2)\ne f(x_3)\)</span>. Hence we have <span class="math inline">\(X_3\)</span> is dependent of <span class="math inline">\(X_2\)</span>.</li>
</ul></li>
<li><strong>To see why the assumption is named “minimality”. We know that if <span class="math inline">\(P\)</span> is Markov with respect to a DAG <span class="math inline">\(G\)</span>, then <span class="math inline">\(P\)</span> satisfies a set of independencies that are specific to the structure of <span class="math inline">\(G\)</span>. If <span class="math inline">\(P\)</span> and <span class="math inline">\(G\)</span> also satisfy minimality, then this set of independencies is minimal in the sense the <span class="math inline">\(P\)</span> does not satisfy any additional independencies. This is equivalent to saying that adjacent nodes are dependent. </strong> (Why?)</li>
</ul>
</div>
<div id="causal-graphs" class="section level3 unnumbered">
<h3>Causal graphs</h3>
<ul>
<li><p>What is a cause? A variable <span class="math inline">\(X\)</span> is said to be a cause of a variable <span class="math inline">\(Y\)</span> if <span class="math inline">\(Y\)</span> can change in response to changes in <span class="math inline">\(X\)</span>. (<strong>Based on the lemma in Appendix A, this definition implies if <span class="math inline">\(X\)</span> is a cause of <span class="math inline">\(Y\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are dependent.</strong>)</p></li>
<li>(Strict) causal edges assumption: In a directed graph, every parent is a direct cause of all its children.
<ul>
<li>Based on the definition of a cause, this assumption implies that adjacent nodes are dependent.</li>
<li>This is indeed the second part of the minimality assumption.</li>
</ul></li>
</ul>
</div>
<div id="two-node-graphs-and-graphical-building-blocks" class="section level3 unnumbered">
<h3>Two-node graphs and graphical building blocks</h3>
<ul>
<li><p>Flow of association: by “flow of association”, we mean whether any two nodes in a graph are associated or not associated. Another way of saying this is whether two nodes are statistically dependent or statistically independent.</p></li>
<li>Two unconnected nodes
<p align="center">
<image src="images/two-unconnected-nodes.png">
</p></li>
</ul>
<p>The Bayesian network factorization (or equivalently the local Markov assumption) implies the two nodes <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are unassociated (independent) in this building block.</p>
<ul>
<li>Two connected nodes
<p align="center">
<image src="images/two-connected-nodes.png">
</p></li>
</ul>
<p>The causal edges assumption implies <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are associated.</p>
</div>
<div id="chains-and-forks" class="section level3 unnumbered">
<h3>Chains and forks</h3>
<p align="center">
<image src="images/fig3-12.png">
</p>
<p align="center">
<image src="images/fig3-13.png">
</p>
<ul>
<li><p>In both chains and forks, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are depenedent, and <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span> are dependent based on the causal edges assumption (or the second part of the minimality assumption).</p></li>
<li>Is <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> associated in chains and forks?
<ul>
<li><strong>Usually, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> are associated in both chains and forks.</strong> Intuitively, in chain graphs, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> are usually dependent simply because <span class="math inline">\(X_1\)</span> causes changes in <span class="math inline">\(X_2\)</span> which then causes changes in <span class="math inline">\(X_3\)</span>. In a fork graph, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> are also usually dependent because the same value that <span class="math inline">\(X_2\)</span> takes on is used to determine both the value that <span class="math inline">\(X_1\)</span> talkes on and the value that <span class="math inline">\(X_3\)</span> takes on.</li>
<li><strong>However, there exist pathological cases where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> are not dependent in chains and forks.</strong> We consider the following example: <span class="math inline">\(P(X=1)=P(X=2)=\frac{1}{2}\)</span>, <span class="math inline">\(P(U_Y=1)=P(U_Y=2)=\frac{1}{2}\)</span>, <span class="math inline">\(X\perp U_Y\)</span>, <span class="math inline">\(Y=3\)</span> if <span class="math inline">\(X=1\)</span> and <span class="math inline">\(U_Y=1\)</span>, <span class="math inline">\(Y=4\)</span> if <span class="math inline">\(X=2\)</span> and <span class="math inline">\(U_Y=1\)</span>, <span class="math inline">\(Y=5\)</span> if <span class="math inline">\(U_Y=2\)</span>; <span class="math inline">\(Z=6\)</span> if <span class="math inline">\(Y=5\)</span> or <span class="math inline">\(U_Z=1\)</span> and <span class="math inline">\(Z=7\)</span> if <span class="math inline">\(Y\ne 5\)</span> and <span class="math inline">\(U_Z=2\)</span>. Then we have
<span class="math display">\[\begin{equation*}
P(X=1, Z=6)=P(X=1, Y=5~\text{or}~U_Z=1)=P(X=1, Y=5, U_Z=1) \\
+P(X=1, Y=5, U_Z=2)+P(X=1, Y=3, U_Z=1)+P(X=1, Y=4, U_Z=1) \\
= \frac{1}{8}+\frac{1}{8}+\frac{1}{8}=\frac{3}{8}=P(X=1)P(Z=6).
\end{equation*}\]</span>
Similarly, we have <span class="math inline">\(P(X=1, Z=7)=P(X=1)P(Z=7)\)</span>, <span class="math inline">\(P(X=2, Z=6)=P(X=2)P(Z=6)\)</span> and <span class="math inline">\(P(X=2, Z=7)=P(X=2)P(Z=7)\)</span>. Therefore, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are independent.</li>
</ul></li>
<li>Blocked paths
<p align="center">
<image src="images/fig3-14-15.png">
</p>
<p>When we condition on <span class="math inline">\(X_2\)</span> in chains and forks, it blocks the flow of association from <span class="math inline">\(X_1\)</span> to <span class="math inline">\(X_3\)</span>. This is because of the local Markov assumption.</p></li>
</ul>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> In chains, we have
<span class="math display">\[\begin{equation*}
f(x_1, x_2, x_3)=f(x_1)f(x_2|x_1)f(x_3|x_2).
\end{equation*}\]</span>
So
<span class="math display">\[\begin{equation*}
f(x_1, x_3|x_2)=\frac{f(x_1)f(x_2|x_1)f(x_3|x_2)}{f(x_2)}=f(x_1|x_2)f(x_3|x_2). 
\end{equation*}\]</span></p>
In forks, we have <span class="math inline">\(f(x_1, x_2, x_3)=f(x_2)f(x_1|x_2)f(x_3|x_2)\)</span>. So
<span class="math display">\[\begin{equation*}
f(x_1, x_3|x_2)=f(x_1|x_2)f(x_3|x_2). 
\end{equation*}\]</span>
</div>

</div>
<div id="colliders-and-their-descendants" class="section level3 unnumbered">
<h3>Colliders and their descendants</h3>
<ul>
<li>In colliders, <span class="math inline">\(X_1\)</span> is independent of <span class="math inline">\(X_3\)</span>. An intuitive thought: <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> simply as unrelated events that happen, which happen to both contribute to some common effect.

<div class="proof">
 <span class="proof"><em>Proof. </em></span> We have <span class="math inline">\(f(x_1, x_2, x_3)=f(x_1)f(x_3)f(x_2|x_1, x_3)\)</span>. So
<span class="math display">\[\begin{equation*}
f(x_1, x_3)=\int f(x_1)f(x_3)f(x_2|x_1, x_3)dx_2=f(x_1)f(x_3). 
\end{equation*}\]</span>
</div>
<p align="center">
<image src="images/fig3-16.png">
</p></li>
<li><strong>Oddly enough, when we condition on the collider <span class="math inline">\(X_2\)</span>, its parents <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> become dependent.</strong>
See the example Good-looking men are jerks from the book.</li>
</ul>
<p>An numerical example: consider the following data generating process <span class="math inline">\(X_1\sim N(0, 1)\)</span>, <span class="math inline">\(X_3\sim N(0, 1)\)</span> and <span class="math inline">\(X_2=X_1+X_3\)</span>. <span class="math inline">\(X_1\)</span>. Then
<span class="math display">\[\begin{equation*}
Cov(X_1, X_3|X_2=x)=E(X_1(x-X_1))=-1,
\end{equation*}\]</span>
where implies that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> are dependent conditoning on <span class="math inline">\(X_2\)</span>.</p>
<p align="center">
<image src="images/fig-3-17.png">
</p>
<ul>
<li>Descendants of colliders: Conditioning on descendants of a collider also induces association in between the parents of the collider. The intuition is that if we learn something about a collider’s descendant, we usually also learn something about the collider itself because there is a direct causal path from the collider to its descendants. For example, consider <span class="math inline">\(X_4=2X_2\)</span> in the above numerical example.</li>
</ul>
</div>
<div id="d-separation" class="section level3 unnumbered">
<h3>d-separation</h3>
<p>to be written…</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interaction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="why-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/duzhewang/cibook/edit/master/06-graphical-representation-of-causal-effects.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/duzhewang/cibook/blob/master/06-graphical-representation-of-causal-effects.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
