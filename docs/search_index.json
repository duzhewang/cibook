[["index.html", "Causal Inference Notes Preface", " Causal Inference Notes Duzhe Wang Last updated date: 2022-01-24 Preface "],["interaction.html", "5. Interaction 5.1 Interaction requires a joint intervention 5.2 Identifying interaction 5.3 Counterfactual response types and interaction 5.4 Sufficient causes 5.5 Sufficient cause interaction 5.6 Counterfactuals or sufficient-component causes?", " 5. Interaction 5.1 Interaction requires a joint intervention We refer to interventions on two or more treatments as joint interventions. We say that there is interaction between \\(A\\) and \\(E\\) on the additive scale in the population if \\[P(Y^{a=1, e=1}=1)-P(Y^{a=0, e=1}=1)\\ne P(Y^{a=1, e=0}=1)-P(Y^{a=0, e=0}=1).\\] This is equivalent to \\[P(Y^{a=1, e=1}=1)-P(Y^{a=1, e=0}=1)\\ne P(Y^{a=0, e=1}=1)-P(Y^{a=0, e=0}=1).\\] Difference between interaction and effect modification: a variable \\(V\\) is a modifier of the effect of \\(A\\) on \\(Y\\) when the average causal effect of \\(A\\) on \\(Y\\) varies across levels of \\(V\\). Note the concept of effect modification refers to the causal effect of \\(A\\), not to the causal effect of \\(V\\). In contrast, the definition of interaction between \\(A\\) and \\(E\\) gives equal status to both treatments \\(A\\) and \\(E\\). The concept of interaction refers to the joint causal effect of two treatments \\(A\\) and \\(E\\), and thus involves the counterfactual outcomes \\(Y^{a, e}\\) under a joint intervention. 5.2 Identifying interaction Consider two cases: Suppose \\(E\\) were randomly, and unconditionally, assigned by the investigators. Then we have \\[P(Y^{a=1, e=1}=1)=P(Y^{a=1}=1|E=1)\\]. Using a similar argument, we can rewrite the definition of interaction between \\(A\\) and \\(E\\) on the additive scale as \\[P(Y^{a=1}=1|E=1)-P(Y^{a=0}=1|E=1)\\ne P(Y^{a=1}=1|E=0)-P(Y^{a=0}=1|E=0).\\] This is exactly the definition of modification of the effect of \\(A\\) by \\(E\\) on the additive scale. In the other words, when treatment \\(E\\) is randomly assigned, then the concepts of interaction and effect modification coincide. Suppose treatment \\(E\\) was not assigned by investigators. To assess the presence of interaction between \\(A\\) and \\(E\\), we need to compute the four marginal risks \\(P(Y^{a, e}=1)\\). An equivalent way of conceptualizing this problem follows: rather than viewing \\(A\\) and \\(E\\) as two distinct treatments with two possible levels (1 or 0) each, one can view \\(AE\\) as a combined treatment with four possible levels \\((11, 01, 10, 00)\\). Under this conceptualization the identification of interaction between two treatments is not different from the identification of the causal effect of one treatment. 5.3 Counterfactual response types and interaction Type a=1, e=1 a=0, e=1 a=1, e=0 a=0, e=0 1 1 1 1 1 2 1 1 1 0 3 1 1 0 1 4 1 1 0 0 5 1 0 1 1 6 1 0 1 0 7 1 0 0 1 8 1 0 0 0 9 0 1 1 1 10 0 1 1 0 11 0 1 0 1 12 0 1 0 0 13 0 0 1 1 14 0 0 1 0 15 0 0 0 1 16 0 0 0 0 Each cell in the above table is \\(Y^{a, e}\\) for each \\(a, e\\) value. Type 1, 4, 6, 11, 13, 16: for an invididual with one of these response types, the causal effect of treatment \\(A\\) on the outcome \\(Y\\) is the same regardless of the value of treatment \\(E\\). Therefore, in a population in which every individual has one of these 6 response types, \\[P(Y^{a=1, e=1}=1)-P(Y^{a=0, e=1}=1)=P(Y^{a=1, e=0}=1)-P(Y^{a=0, e=0}=1).\\] That is, if all individuals in the population have responses 1, 4, 6, 11, 13 and 16, then there will be no interaction between \\(A\\) and \\(E\\) on the additive scale. The presence of additive interaction between \\(A\\) and \\(E\\) implies that there must be individuals in at least one of the other remaining response types. 5.4 Sufficient causes Sufficient-component causes 9 possible sufficient-component causes for a dichotomous outcome and two treatments: 5.5 Sufficient cause interaction The definition of interaction within the counterfactual framework does not require any knowledge about those mechanisms nor even that the treatments work together. Another concept of interaction is not based on counterfactual contrasts but rather on sufficient-component causes. Therefore, it is called interaction within the sufficient-component-cause framework, or for brevity, sufficient cause interaction. A sufficient cause interaction between \\(A\\) and \\(E\\) exists in the population if \\(A\\) and \\(E\\) occur together in a sufficient cause. For example, suppose individuals with background factors \\(U_5=1\\) will develop the outcome when jointly receiving \\(E=1\\) and \\(A=1\\), but not when receiving only one of the two treatments. Then a sufficient cause interaction between \\(A\\) and \\(E\\) exists if there exists an individual with \\(U_5=1\\). 5.6 Counterfactuals or sufficient-component causes? The sufficient component cause model considers sets of actions, events, or states of nature which together inevitably bring about the outcome under consideration. The model gives an account of the causes of a particular effect. It addresses the question, “Given a particular effect, what are the various events which might have been its cause?” The potential outcomes focus on one particular cause or intervention and gives an account of the various effects of that cause. In contrast to the sufficient component cause framework, the potential outcomes framework addresses the question, “What would have occurred if a particular factor were intervened upon and thus set to a different level than it in fact was?” Unlike the sufficient component cause framework, the counterfactual framework does not require a detailed knowledge of the meachanisms by which the factor affects the outcome. "],["graphical-representation-of-causal-effects.html", "6. Graphical representation of causal effects 6.1 Causal diagrams 6.2 Causal diagrams and marginal independence 6.3 Causal diagrams and conditional independence Appendix A: uncorrelated vs. independent Appendix B: The flow of association and causation in graphs", " 6. Graphical representation of causal effects The use of graphs in causal inference problems makes it easier to follow a sensible advice: draw your assumptions before your conclusions. 6.1 Causal diagrams Causal directed acyclic graphs We define a DAG \\(G\\) to be a graph whose nodes are random variables \\(V=(V_1,..., V_M)\\) with directed edges and no directed cycles. We use \\(PA_m\\) to denote the parents of \\(V_m\\), i.e., the set of nodes from which there is a direct arrow into \\(V_m\\). The variable \\(V_m\\) is a descendant of \\(V_j\\) if there is a sequence of nodes connected by edges between \\(V_j\\) and \\(V_m\\) such that, following the direction indicated by the arrows, one can reach \\(V_m\\) by starting at \\(V_j\\). We adopt the ordering convention that if \\(m&gt;j\\), \\(V_m\\) is not an ancestor of \\(V_j\\). We define the distribution of \\(V\\) to be Markov with respect to a DAG \\(G\\) if for each \\(j\\), \\(V_j\\) is independent of its non-descendants conditional on its parents. A causal DAG is a DAG in which 1) the lack of an arrow from node \\(V_j\\) to \\(V_m\\) can be interpreted as the absence of a direct causal effect of \\(V_j\\) on \\(V_m\\) relative to the other variables on the graph, 2) all common causes, even if unmeasured, of any pair of variables on the graph are themselves on the graph, and 3) any variable is a cause of its descendants. Causal Markov assumption: conditional on its direct causes, a variable \\(V_j\\) is independent of any variable for which it is not a cause. That is, conditional on its parents, \\(V_j\\) is independent of its non-descendants. Mathematically, it’s equivalent to the statement that the density \\(f(V)\\) of the variables \\(V\\) in DAG \\(G\\) satisfies the Markov factorization \\[ f(v)=\\prod_{i=1}^{M}f(v_j|pa_{j}). \\] \\iffalse{} Proof. We want to show the equivalence. We have \\[f(v)=f(v_1)\\prod_{i=2}^{M}f(v_j|v_1,..., v_{j-1} ).\\] Based on the Markov assumption and the ordering convention, we have \\[f(v_j|v_1,..., v_{j-1})=f(v_j|pa_{j}).\\] Therefore, we have \\(f(v)=\\prod_{i=1}^{M}f(v_j|pa_{j})\\). Theorem 20.3 in the book All of Statistics also has an explanation. Examples A marginally randomized experiment can be represented by the following causal DAG: For example, if we know that aspirin use \\(A\\) has a preventive causal effect on the risk of heart disease \\(Y\\), i.e., \\(P(Y^{a=1}=1)\\ne P(Y^{a=0}=1)\\). The causal diagram in Figure 6.2 is the graphical translation of this knowledge for an experiment in which aspirin \\(A\\) is randomly, and unconditionally, assigned. A conditionally randomized experiment can be represented by the following causal DAG: Note this figure can also represent an observational study. Figure 6.1 represents an observational study in which we are willing to assume that the assignment of \\(A\\) has \\(L\\) as parent and no other causes of \\(Y\\). Otherwise, those causes of \\(Y\\), even if unmeasured, would need to be included in the diagram, as they would be common causes of \\(A\\) and \\(Y\\). Suppose we know that carrying a lighter \\(A\\) has no causal effect on anyone’s risk of lung cancer \\(Y\\), i.e., \\(P(Y^{a=1}=1)=P(Y^{a=0}=1)\\), and that cigarette smoking \\(L\\) hs a causal effect on both carrying a lighter \\(A\\) and lung cancer \\(Y\\). The causal diagram in Figure 6.3 is the graphical translation of this knowledge. 6.2 Causal diagrams and marginal independence Causal diagrams are a simple way to encode our subject-matter knowledge, and our assumptions, about the qualitative causal structure of a problem. Causal diagrams also encode information about potential associations between the variables in the causal network. It is precisely this simultaneous representation of association and causation that makes causal diagrams such an attractive tool. When one knows that \\(A\\) has a causal effect on \\(Y\\), as in Figure 6.2, then one should also generally expect \\(A\\) and \\(Y\\) to be associated. This is consistenct with the fact that, in an ideal randomized experiment with unconditional exchangeability, causation \\(P(Y^{a=1}=1)\\ne P(Y^{a=0}=1)\\) implies association \\(P(Y=1|A=1)\\ne P(Y=1|A=0)\\), and vice versa. In Figure 6.3, we have \\(P(Y^{a=1}=1)=P(Y^{a=0}=1)\\). But \\(P(Y=1|A=1)\\ne P(Y=1|A=0)\\). See the book for an intuitive explanation. Consider the following causal DAG: The common effect \\(L\\) is referred to as a collider on the path \\(A\\rightarrow L\\leftarrow Y\\) because two arrowheads collide on this node. Colliders, unlike other variables, block the flow of association along the path on which they lie. Thus, \\(A\\) and \\(Y\\) are independent because the only path between them, \\(A\\rightarrow L\\leftarrow Y\\), is blocked by the collider \\(L\\). \\iffalse{} Proof. By Markov assumption, we have \\[ f(a, y, l)=f(a)f(y)f(l|a, y).\\] So \\[f(a, y)=\\int f(a, y, l)dl=\\int f(a)f(y)f(l|a, y)dl=f(a)f(y).\\] Therefore, \\(A\\) and \\(Y\\) are independent. In summary, two variables are marginally associated if one causes the other, or if they share common causes. Otherwise they will be marginally independent. 6.3 Causal diagrams and conditional independence Consider the following causal DAG: Question: is there an association between \\(A\\) and \\(Y\\) within levels of (conditional on) \\(B\\)? Conclusion: Even though \\(A\\) and \\(Y\\) are marginally associated, \\(A\\) and \\(Y\\) are conditionally independent given \\(B\\). Graphically, we say that a box placed around variable \\(B\\) blocks the flow of association through the path \\(A\\rightarrow B\\rightarrow Y\\). \\iffalse{} Proof. By Markov assumption (conditional on its direct causes, a variable \\(V_j\\) is independent of any variable for which it is not a cause), we have \\(Y\\) is independent of \\(A\\) conditional on \\(B\\). Consider the following causal DAG: Question: is \\(A\\) associated with \\(Y\\) conditional on \\(L\\)? Conclusion: \\(A\\) and \\(Y\\) are conditionally independent given \\(L\\). \\iffalse{} Proof. By Markov assumption. Consider the following causal DAG: Question: Is \\(A\\) associated with \\(Y\\) conditional on \\(L\\)? Conclusion: Appendix A: uncorrelated vs. independent Two random variables \\(X\\) and \\(Y\\) are uncorrelated when their correlation coefficient is 0: \\(\\rho(X, Y)=0\\). Being uncorrelated is the same as having zero covariance. If \\(\\rho(X, Y)\\ne 0\\), then \\(X\\) and \\(Y\\) are correlated. Two random variables are independent when their joint probability distribution is the product of their marginal probability distributions: for all \\(x\\) and \\(y\\), \\[p_{X, Y}(x, y)=p_{X}(x)p_{Y}(y).\\] Equivalently, the conditional distribution is the same as the marginal distribution: \\[p_{Y|X}(y|x)=P_{Y}(y).\\] If \\(X\\) and \\(Y\\) are not independent, then they are dependent. If \\(Y\\) is a non-constant function of \\(X\\), then \\(X\\) and \\(Y\\) are always dependent. (Lemma: Let \\(X\\) be a random variable and let \\(f\\) be a Borel measurable function such that \\(X\\) and \\(f(X)\\) are independent. Then \\(f(X)\\) is constant almost surely. That is, there is some \\(a\\in \\mathbb{R}\\) such that \\(P(f(X)=a)=1\\).) \\iffalse{} Proof. See https://stats.stackexchange.com/questions/16321/are-the-random-variables-x-and-fx-dependent. If \\(X\\) and \\(Y\\) are independent, then they are also uncorrelated. (This also implies that if \\(X\\) and \\(Y\\) are correlated, then they are dependent.) \\[\\begin{equation*} E(XY)=\\int\\int xyp_{X, Y}(x, y)dxdy = \\int\\int xyp_{X}(x)p_{Y}(y)dxdy \\\\ = \\int xp_{X}(x)dx\\int yp_{Y}(y)dy =E(X)E(Y) \\end{equation*}\\] However, if \\(X\\) and \\(Y\\) are uncorrelated, then they can still be dependent. For example, let \\(Y=X^2\\), then if \\(E(X)=0\\) and \\(E(X^3)=0\\), we have \\(Cov(X, Y)=E(XY)=E(X^3)=0\\), so \\(X\\) and \\(Y\\) are uncorrelated, but they are dependent based on the above lemma. Take-away message: Pearson correlation only measures the linear dependence. Appendix B: The flow of association and causation in graphs This section is mainly from Brady Neal’s Introduction to Causal Inference from a Machine Learning Perspective. Graph terminology If two parents \\(X\\) and \\(Y\\) share some child \\(Z\\), but there is no edge connecting \\(X\\) and \\(Y\\), then \\(X\\rightarrow Z\\leftarrow Y\\) is known as an immorality. Bayesian networks Chain rule: \\(P(x_1, x_2,..., x_n)=P(x_1)\\prod_i P(x_i|x_{i-1},...,x_1)\\) Given a probability distribution and a corresponding DAG, we can formalize the specification of independencies with the local Markov assumption: Given its parents in the DAG, a node \\(X\\) is independent of its non-descendants. A probability distribution is said to be locally Markov with respect to a DAG if they satisfy the local Markov assumption. Bayesian network factorization: Given a probability distribution P and a DAG G, P factorizes according to \\(G\\) if \\(P(x_1,..., x_n)=\\prod_i P(x_i|pa_i)\\). Local Markov assumption and Bayesian network factorization are equivalent. The local Markov assumption only gives us information about the independencies in P that a DAG implies. It does not even tell us that if \\(X\\) and \\(Y\\) are adjacent in the DAGs, then \\(X\\) and \\(Y\\) are dependent. And this additional information is very commonly assumed in causal DAGs.To get the guaranteed dependence between adjacent nodes, we generally assume a slightly stronger assumption than the local Markov assumption, which is called the minimality assumption: Local Markov assumption Adjacent nodes in the DAG are dependent Note the second part of minimality assumption is equivalent to the following way: if we were to remove any edges from the DAG, P would not be Markov with respect to the graph with the removed edges. We show this equivalence by a concrete example: see the following DAG. We first show part 2 of the minimality assumption implies the above statement. Based on the local Markov assumption, we have \\[f(x_1, x_2, x_3, x_4)=f(x_1)f(x_2|x_1)f(x_3|x_1, x_2)f(x_4|x_3). \\] Consider the case that we remove the edge \\(X_2\\rightarrow X_3\\). If \\(P\\) is Markov with respect to the graph with the removed edge, then we have \\[f(x_1, x_2, x_3, x_4)=f(x_1)f(x_2|x_1)f(x_3|x_1)f(x_4|x_3).\\] Therefore, we have \\[f(x_3|x_1, x_2)=f(x_3|x_1),\\] which implies that \\(f(x_3|x_2)=f(x_3)\\). Then \\(X_3\\) is independent of \\(X_2\\). This contracts with part 2 of the minimality assumption. So we have \\(P\\) is not Markov with respect to the graph with the removed edges. Next, we show the other direction. Since \\(P\\) would not be Markov respect to the graph with the removed edges, so we have \\(f(x_3|x_1, x_2)\\ne f(x_3|x_1)\\), which implies that \\(f(x_3|x_2)\\ne f(x_3)\\). Hence we have \\(X_3\\) is dependent of \\(X_2\\). To see why the assumption is named “minimality”. We know that if \\(P\\) is Markov with respect to a DAG \\(G\\), then \\(P\\) satisfies a set of independencies that are specific to the structure of \\(G\\). If \\(P\\) and \\(G\\) also satisfy minimality, then this set of independencies is minimal in the sense the \\(P\\) does not satisfy any additional independencies. This is equivalent to saying that adjacent nodes are dependent. (Why?) Causal graphs What is a cause? A variable \\(X\\) is said to be a cause of a variable \\(Y\\) if \\(Y\\) can change in response to changes in \\(X\\). (Based on the lemma in Appendix A, this definition implies if \\(X\\) is a cause of \\(Y\\), then \\(X\\) and \\(Y\\) are dependent.) (Strict) causal edges assumption: In a directed graph, every parent is a direct cause of all its children. Based on the definition of a cause, this assumption implies that adjacent nodes are dependent. This is indeed the second part of the minimality assumption. Two-node graphs and graphical building blocks Flow of association: by “flow of association”, we mean whether any two nodes in a graph are associated or not associated. Another way of saying this is whether two nodes are statistically dependent or statistically independent. Two unconnected nodes The Bayesian network factorization (or equivalently the local Markov assumption) implies the two nodes \\(X_1\\) and \\(X_2\\) are unassociated (independent) in this building block. Two connected nodes The causal edges assumption implies \\(X_1\\) and \\(X_2\\) are associated. Chains and forks In both chains and forks, \\(X_1\\) and \\(X_2\\) are depenedent, and \\(X_2\\) and \\(X_3\\) are dependent based on the causal edges assumption (or the second part of the minimality assumption). Is \\(X_1\\) and \\(X_3\\) associated in chains and forks? Usually, \\(X_1\\) and \\(X_3\\) are associated in both chains and forks. Intuitively, in chain graphs, \\(X_1\\) and \\(X_3\\) are usually dependent simply because \\(X_1\\) causes changes in \\(X_2\\) which then causes changes in \\(X_3\\). In a fork graph, \\(X_1\\) and \\(X_3\\) are also usually dependent because the same value that \\(X_2\\) takes on is used to determine both the value that \\(X_1\\) talkes on and the value that \\(X_3\\) takes on. However, there exist pathological cases where \\(X_1\\) and \\(X_3\\) are not dependent in chains and forks. We consider the following example: \\(P(X=1)=P(X=2)=\\frac{1}{2}\\), \\(P(U_Y=1)=P(U_Y=2)=\\frac{1}{2}\\), \\(X\\perp U_Y\\), \\(Y=3\\) if \\(X=1\\) and \\(U_Y=1\\), \\(Y=4\\) if \\(X=2\\) and \\(U_Y=1\\), \\(Y=5\\) if \\(U_Y=2\\); \\(Z=6\\) if \\(Y=5\\) or \\(U_Z=1\\) and \\(Z=7\\) if \\(Y\\ne 5\\) and \\(U_Z=2\\). Then we have \\[\\begin{equation*} P(X=1, Z=6)=P(X=1, Y=5~\\text{or}~U_Z=1)=P(X=1, Y=5, U_Z=1) \\\\ +P(X=1, Y=5, U_Z=2)+P(X=1, Y=3, U_Z=1)+P(X=1, Y=4, U_Z=1) \\\\ = \\frac{1}{8}+\\frac{1}{8}+\\frac{1}{8}=\\frac{3}{8}=P(X=1)P(Z=6). \\end{equation*}\\] Similarly, we have \\(P(X=1, Z=7)=P(X=1)P(Z=7)\\), \\(P(X=2, Z=6)=P(X=2)P(Z=6)\\) and \\(P(X=2, Z=7)=P(X=2)P(Z=7)\\). Therefore, \\(X\\) and \\(Z\\) are independent. Blocked paths When we condition on \\(X_2\\) in chains and forks, it blocks the flow of association from \\(X_1\\) to \\(X_3\\). This is because of the local Markov assumption. \\iffalse{} Proof. In chains, we have \\[\\begin{equation*} f(x_1, x_2, x_3)=f(x_1)f(x_2|x_1)f(x_3|x_2). \\end{equation*}\\] So \\[\\begin{equation*} f(x_1, x_3|x_2)=\\frac{f(x_1)f(x_2|x_1)f(x_3|x_2)}{f(x_2)}=f(x_1|x_2)f(x_3|x_2). \\end{equation*}\\] In forks, we have \\(f(x_1, x_2, x_3)=f(x_2)f(x_1|x_2)f(x_3|x_2)\\). So \\[\\begin{equation*} f(x_1, x_3|x_2)=f(x_1|x_2)f(x_3|x_2). \\end{equation*}\\] Colliders and their descendants In colliders, \\(X_1\\) is independent of \\(X_3\\). An intuitive thought: \\(X_1\\) and \\(X_3\\) simply as unrelated events that happen, which happen to both contribute to some common effect. \\iffalse{} Proof. We have \\(f(x_1, x_2, x_3)=f(x_1)f(x_3)f(x_2|x_1, x_3)\\). So \\[\\begin{equation*} f(x_1, x_3)=\\int f(x_1)f(x_3)f(x_2|x_1, x_3)dx_2=f(x_1)f(x_3). \\end{equation*}\\] Oddly enough, when we condition on the collider \\(X_2\\), its parents \\(X_1\\) and \\(X_3\\) become dependent. See the example Good-looking men are jerks from the book. An numerical example: consider the following data generating process \\(X_1\\sim N(0, 1)\\), \\(X_3\\sim N(0, 1)\\) and \\(X_2=X_1+X_3\\). \\(X_1\\). Then \\[\\begin{equation*} Cov(X_1, X_3|X_2=x)=E(X_1(x-X_1))=-1, \\end{equation*}\\] where implies that \\(X_1\\) and \\(X_3\\) are dependent conditoning on \\(X_2\\). Descendants of colliders: Conditioning on descendants of a collider also induces association in between the parents of the collider. The intuition is that if we learn something about a collider’s descendant, we usually also learn something about the collider itself because there is a direct causal path from the collider to its descendants. For example, consider \\(X_4=2X_2\\) in the above numerical example. d-separation to be written… "],["why-model.html", "11. Why model? Some concepts and points Program 11.1 Program 11.2 Program 11.3", " 11. Why model? Some concepts and points Estimand Estimator We can not always let the data “speak for themselves” to obtain a meaningful estimate. Rather, we often need to supplement the data with a model. What is a model? A model is defined by an a priori restriction on the joint distribution of the data. When using a parametric model, the inferences are correct only if the restrictions encoded in the model are correct, i.e., if the model is correctly specified. Thus model-based causal inference relies on conditions of no model misspecification. Fisher consistency: An estimator of a population quantity that, when calculated using the entire population rather than a sample, yields the true value of the population parameter. Bias-variance trade-off Program 11.1 Sample averages by treatment level Data from Figures 11.1 and 11.2 A&lt;-c(rep(1, 8), rep(0, 8)) Y &lt;- c(200, 150, 220, 110, 50, 180, 90, 170, 170, 30, 70, 110, 80, 50, 10, 20) plot(A, Y, pch=16) mean(Y[A == 0]) ## [1] 67.5 mean(Y[A == 1]) ## [1] 146 A2&lt;-c(rep(1,4), rep(2, 4), rep(3, 4), rep(4,4)) Y2 &lt;- c(110, 80, 50, 40, 170, 30, 70, 50, 110, 50, 180, 130, 200, 150, 220, 210) plot(A2, Y2, pch=16) mean(Y2[A2 == 1]) ## [1] 70 mean(Y2[A2 == 2]) ## [1] 80 mean(Y2[A2 == 3]) ## [1] 118 mean(Y2[A2 == 4]) ## [1] 195 Program 11.2 2-parameter linear model Data from Figures 11.3 and 11.1 A3 &lt;-c(3, 11, 17, 23, 29, 37, 41, 53, 67, 79, 83, 97, 60, 71, 15, 45) Y3 &lt;-c(21, 54, 33, 101, 85, 65, 157, 120, 111, 200, 140, 220, 230, 217, 11, 190) plot(Y3 ~ A3, pch=16) summary(glm(Y3 ~ A3)) ## ## Call: ## glm(formula = Y3 ~ A3) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -61.93 -30.56 -5.74 30.65 77.22 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.55 21.33 1.15 0.2691 ## A3 2.14 0.40 5.35 0.0001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1944) ## ## Null deviance: 82800 on 15 degrees of freedom ## Residual deviance: 27218 on 14 degrees of freedom ## AIC: 170.4 ## ## Number of Fisher Scoring iterations: 2 predict(glm(Y3 ~ A3), data.frame(A3 = 90)) ## 1 ## 217 summary(glm(Y ~ A)) ## ## Call: ## glm(formula = Y ~ A) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -96.25 -40.00 3.12 35.94 102.50 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 67.5 19.7 3.42 0.0041 ** ## A 78.8 27.9 2.82 0.0135 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 3110) ## ## Null deviance: 68344 on 15 degrees of freedom ## Residual deviance: 43538 on 14 degrees of freedom ## AIC: 177.9 ## ## Number of Fisher Scoring iterations: 2 Program 11.3 3-parameter linear model: \\(E(Y|A)=\\theta_0+\\theta_{1}A+\\theta_{2}A^2\\), where \\(A^2=A\\times A\\) Data from Figure 11.3 Asq &lt;- A3 * A3 mod3 &lt;- glm(Y3 ~ A3 + Asq) summary(mod3) ## ## Call: ## glm(formula = Y3 ~ A3 + Asq) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -65.3 -34.4 13.2 26.1 64.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.4069 31.7478 -0.23 0.819 ## A3 4.1072 1.5309 2.68 0.019 * ## Asq -0.0204 0.0153 -1.33 0.206 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1843) ## ## Null deviance: 82800 on 15 degrees of freedom ## Residual deviance: 23955 on 13 degrees of freedom ## AIC: 170.4 ## ## Number of Fisher Scoring iterations: 2 predict(mod3, data.frame(cbind(A3 = 90, Asq = 8100))) ## 1 ## 197 "],["ip-weighting-and-marginal-structural-models.html", "12. IP weighting and marginal structural models Part 1: Summary of the chapter Part 2: Real data analysis", " 12. IP weighting and marginal structural models Part 1: Summary of the chapter Definitions An individual’s IP weight depends on her values of treatment \\(A\\) and covariate \\(L\\). Since the denominator of the weight for each individual is the conditional density evaluated at the individual’s own values of \\(A\\) and \\(L\\), it can be expressed as the conditional density evaluated at the random arguments \\(A\\) and \\(L\\), that is, as \\(f(A|L)\\). Therefore, we write the IP weights as \\(1/f(A|L)\\). Potential outcome mean for treatment level \\(A=a\\): \\(E(Y^{a})\\) Standardized mean for treatment level \\(A=a\\): \\(\\sum_{l}E(Y|A=a, L=l)P(L=l)\\) IP weighted mean of \\(Y\\) for treatment level \\(A=a\\): \\(E\\left(\\frac{I(A=a)Y}{f(A|L)}\\right)\\) 0.0.1 More about conditional exchangeability assumption First, the conditional exchangeability assumption is also called unconfoundedness. The latter name is much easier to understand the meaning of this assumption Next, why do we call it conditional exchangeability? Conditional exchangeability means condtionining on \\(X\\), the treatment group is comparable to the control group. More precisely, it means \\(f(Y^{(a=1)}|A=0, X)=f(Y^{(a=1)}|A=1, X)\\). It is equivalent to \\(Y^{(a=1)}\\perp A|X\\). Therefore, condtional exchangeability and unconfoundedness are the same thing. Third, unconfoundedness implies no unobserved/unmeasured confounders. Suppose there exists unmeasured confounding \\(X_u\\), then we have \\(Y^{(a=0)}=f_0(X, X_u)\\), \\(Y^{(a=1)}=f_1(X, X_u)\\) and \\(A=f_A(X, X_u)\\). Therefore, only conditioning on \\(X\\) would not make unconfoundedness satisfied. Since unconfoundedness is the key assumption to identify the causal effect, so based on the above reasoning, when we assume the unconfoundness, we also implicitly assume no unmeasured confounders. Equivalence of IP weighting and standardization Under the positivity assumption, we have \\[\\begin{equation*} \\begin{split} E\\left(\\frac{I(A=a)Y}{f(A|L)}\\right)=E_{A, L}\\left( E_{Y}(\\frac{I(A=a)}{f(A|L)}Y|A, L)\\right)=E_{A, L}\\left( E_{Y}(Y|A, L)\\frac{I(A=a)}{f(A|L)}\\right) \\\\ = E_{L}\\left(E_{A}\\left( E_{Y}(Y|A, L)\\frac{I(A=a)}{f(A|L)}|L\\right)\\right)=E_{L}\\left( E_{Y}(Y|A=a, L)|L\\right) \\\\ =\\sum_{l}E(Y|A=a, L=l)P(L=l) \\end{split} \\end{equation*}\\] Note: we require the positivity to make sure that \\(\\frac{I(A=a)}{f(A|L)}\\) is well-defined. Equivalence of potential outcome mean, standardized mean and IP weighted mean First, we show \\(E(Y^a)=\\sum_{l}E(Y|A=a, L=l)P(L=l)\\) under conditional exchangeability, positivity, and consistency. \\iffalse{} Proof. \\[\\begin{equation*} \\begin{split} E(Y^{a})=\\sum_{l}E(Y^a|L=l)P(L=l) \\\\ =\\sum_{l}E(Y^a|A=a, L=l)P(L=l) \\\\ =\\sum_{l}E(Y|A=a, L=l)P(L=l), \\end{split} \\end{equation*}\\] where the second equality is by conditional exchangeability and positivity, and the third by consistency. Second, we show \\(E(Y^a)=E\\left( \\frac{I(A=a)}{f(A|L)}Y \\right)\\) under positivity, conditional exchangeability, and consistency. \\iffalse{} Proof. \\[\\begin{equation*} \\begin{split} E\\left(\\frac{I(A=a)}{f(A|L)}Y\\right)= E\\left( \\frac{I(A=a)}{f(A|L)}Y^a \\right)=E\\left( E\\left(\\frac{I(A=a)}{f(A|L)}Y^{a}|L\\right) \\right) \\\\ = E\\left(E\\left(\\frac{I(A=a)}{f(A|L)}|L\\right)E\\left(Y^a|L\\right) \\right)=E\\left(1\\times E\\left(Y^{a}|L\\right) \\right)=E(Y^a), \\end{split} \\end{equation*}\\] where the first equality is by consistency, the third by conditional exchangeability. Furthermore, we need positivity to well define the IP weights. What does IP weighting mean? IP weighting creates a pseudo-population in which the arrow from the covariates \\(L\\) to the treatment \\(A\\) is removed (ther is no association between covariates \\(L\\) and the treatment \\(A\\)). The pseudo-population has the following two properties: - \\(A\\) and \\(L\\) are statistically independent - The mean \\(E_{ps}(Y|A=a):=E\\left(\\frac{I(A=a)}{f(A|L)}Y\\right)\\) in the pseudo-population equals the standardized mean \\(\\sum_{l}E(Y|A=a, L=l)P(L=l)\\) in the actual population under positivity. (See the above section equivalence of IP weighting and standardization for a detailed proof.) An example The following example illustrates the fact that inverse probability weighting removes confounding by creating a pseudo-population in which the treatment is independent of the measured confounders. The original population: A=0 A=1 Total L=0 P(A=0|L=0)a P(A=1|L=0)a a L=1 P(A=0|L=1)b P(A=1|L=1)b b a+b The pseudo-population: A=0 A=1 Total L=0 a a 2a L=1 b b 2b 2(a+b) From the above table, we can find in the pseudo-population, \\[P_{ps}(A=0|L=0)=P_{ps}(A=0|L=1)=P_{ps}(A=0)=\\frac{1}{2}\\] and \\[P_{ps}(A=1|L=0)=P_{ps}(A=1|L=1)=P_{ps}(A=1)=\\frac{1}{2}.\\] The IP weights simulate a pseudo-population in which all members of the original population are replaced by two copies of themselves. One copy receives treatment \\(A=1\\) and the other copy receives treatment value \\(A=0\\). Horvitz-Thompson estimator and Hajek estimator The IP weighted mean can be consistently estimated by Horvitz-Thompson estimator \\(\\widehat{E}(\\frac{I(A=a)}{f(A|L)}Y)\\) Note under positivity, \\(\\frac{E(\\frac{I(A=a)}{f(A|L)}Y)}{E(\\frac{I(A=a)}{f(A|L)})}=E(\\frac{I(A=a)}{f(A|L)}Y)\\). Hence, the Hajek estimator \\(\\frac{\\widehat{E}(\\frac{I(A=a)}{f(A|L)}Y)}{\\widehat{E}(\\frac{I(A=a)}{f(A|L)})}\\) can also be used to estimate the IP weighted mean. In practice, the Hajek estimator is preferred because it is guaranteed to lie between 0 and 1 for dichotomous \\(Y\\). Stablized IP weights Several facts from mathematics: under positivity, conditional exchangeability, and consistency, \\[\\begin{equation*} \\frac{E(\\frac{pI(A=a)}{f(A|L)}Y)}{E(\\frac{pI(A=a)}{f(A|L)})}=E(Y^a), \\end{equation*}\\] and \\[\\begin{equation*} \\frac{E(\\frac{g(A)I(A=a)}{f(A|L)}Y)}{E(\\frac{g(A)I(A=a)}{f(A|L)})}=E(Y^a), \\end{equation*}\\] where g(A) is any function of \\(A\\) that is not a function of \\(L\\). Usually we take \\(g(A)=f(A)\\). The IP weights \\(W^{A}=1/f(A|L)\\) are referred to as nonstabilized weights, and the IP weights \\(SW^{A}=f(A)/f(A|L)\\) are referred to as stabilized weights. The stablizing factor \\(f(A)\\) in the numerator is responsible for the narrower range of the \\(f(A)/f(A|L)\\) weights. More on IPW: http://www.rebeccabarter.com/blog/2017-07-05-ip-weighting/ Marginal structural models Examples of marginal structural mean models: \\[E(Y^a)=\\beta_0+\\beta_1a\\] or \\[E(Y^a)=\\beta_0+\\beta_1a+\\beta_2a^2. \\] How do we use marginal structural models? If we assume the marginal structural mean model \\(E(Y^a)=\\beta_0^*+\\beta_1^*a\\), then we have \\[E\\left(\\frac{I(A=a)}{f(A|L)}Y\\right)=E(Y^a)=\\beta_0^*+\\beta_1^*a.\\] Furthermore, it is easy to show that \\[\\begin{equation*} E\\left(\\frac{I(A=a)}{f(A|L)}Y\\right)=\\text{argmin}_{x}E\\left( \\frac{I(A=a)}{f(A|L)}(Y-x)^2 \\right). \\end{equation*}\\] Therefore, we have \\[\\begin{equation*} \\beta^*_0, \\beta^*_1=\\text{argmin}_{\\beta_0, \\beta_1}E\\left( \\frac{I(A=a)}{f(A|L)}(Y-\\beta_0-\\beta_1a)^2 \\right). \\end{equation*}\\] Hence, in the finite sample case, we estimate \\(\\beta_0^*\\) and \\(\\beta_1^*\\) by the following optimization problem: \\[\\begin{equation*} \\hat{\\beta}_0, \\hat{\\beta}_1=\\text{argmin}_{\\beta_0, \\beta_1}\\sum_{i=1}^{n} \\widehat{W}_i(Y_i-\\beta_0-\\beta_1a)^2, \\end{equation*}\\] where \\(\\widehat{W}_i\\) is the estimated IP weight. Using a similar argument with the above section, we can estimate the model parameters by fitting the linear regression model Effect modification and marginal structural models We may include covariates-which may be non-confounders-in a marginal structural model to assess effect modification. Suppose it is hypothesized that the effect of smoking cessation varies by sex V (1: woman, 0: man). To examine this hypothesis, we can assume \\[E(Y^a|V)=\\beta_0+\\beta_1a+\\beta_2Va+\\beta_3V.\\] Then we have \\[E(Y^{a=1}|V=1)-E(Y^{a=0}|V=1)=\\beta_1+\\beta_2\\] and \\[E(Y^{a=1}|V=0)-E(Y^{a=0}|V=0)=\\beta_1.\\] Therefore, additive effect modification is present if \\(\\beta_2\\ne 0\\). Part 2: Real data analysis Background NHEFS data (National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study) Goal: estimate the effect of smoking cessation on weight gain \\(A=1\\): if cigarette smokers reported having quit smoking before the follow-up visit \\(Y\\): the body weight at the follow-up visit minus the body weight at the baseline visit \\(E(Y^{a=1})\\): mean weight gain that would have been observed if all individuals in the population had quit smoking before the follow-up visit \\(E(Y^{a=0})\\): mean weight gain that would have been observed if all individuals in the population had not quit smoking Average causal effect: \\(E(Y^{a=1})-E(Y^{a=0})\\) library(here) # for path library(readxl) # for reafing excel files library(tidyverse) library(hrbrthemes) library(viridis) library(geepack) # for GEE # copy from https://www.r-graph-gallery.com/89-box-and-scatter-plot-with-ggplot2.html Input dataset nhefs=read_excel(here(&quot;data&quot;, &quot;NHEFS.xls&quot;)) head(nhefs) ## # A tibble: 6 × 64 ## seqn qsmk death yrdth modth dadth sbp dbp sex age race income marital school education ht wt71 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 233 0 0 NA NA NA 175 96 0 42 1 19 2 7 1 174. 79.0 ## 2 235 0 0 NA NA NA 123 80 0 36 0 18 2 9 2 159. 58.6 ## 3 244 0 0 NA NA NA 115 75 1 56 1 15 3 11 2 168. 56.8 ## 4 245 0 1 85 2 14 148 78 0 68 1 15 3 5 1 170. 59.4 ## 5 252 0 0 NA NA NA 118 77 0 40 0 18 2 11 2 182. 87.1 ## 6 257 0 0 NA NA NA 141 83 1 43 1 11 4 9 2 162. 99 ## # … with 47 more variables: wt82 &lt;dbl&gt;, wt82_71 &lt;dbl&gt;, birthplace &lt;dbl&gt;, smokeintensity &lt;dbl&gt;, ## # smkintensity82_71 &lt;dbl&gt;, smokeyrs &lt;dbl&gt;, asthma &lt;dbl&gt;, bronch &lt;dbl&gt;, tb &lt;dbl&gt;, hf &lt;dbl&gt;, hbp &lt;dbl&gt;, ## # pepticulcer &lt;dbl&gt;, colitis &lt;dbl&gt;, hepatitis &lt;dbl&gt;, chroniccough &lt;dbl&gt;, hayfever &lt;dbl&gt;, diabetes &lt;dbl&gt;, ## # polio &lt;dbl&gt;, tumor &lt;dbl&gt;, nervousbreak &lt;dbl&gt;, alcoholpy &lt;dbl&gt;, alcoholfreq &lt;dbl&gt;, alcoholtype &lt;dbl&gt;, ## # alcoholhowmuch &lt;dbl&gt;, pica &lt;dbl&gt;, headache &lt;dbl&gt;, otherpain &lt;dbl&gt;, weakheart &lt;dbl&gt;, allergies &lt;dbl&gt;, ## # nerves &lt;dbl&gt;, lackpep &lt;dbl&gt;, hbpmed &lt;dbl&gt;, boweltrouble &lt;dbl&gt;, wtloss &lt;dbl&gt;, infection &lt;dbl&gt;, active &lt;dbl&gt;, ## # exercise &lt;dbl&gt;, birthcontrol &lt;dbl&gt;, pregnancies &lt;dbl&gt;, cholesterol &lt;dbl&gt;, hightax82 &lt;dbl&gt;, price71 &lt;dbl&gt;, … Ignore subjects with missing values for weight in 1982 wt82: weight in 1982 qsmk: quit smoking between 1st questionnaire and 1982 (Yes:1; No:0) nhefs.nmv=nhefs[which(!is.na(nhefs$wt82)), ] nhefs.nmv$qsmk=as.factor(nhefs.nmv$qsmk) Compare the treatment group and the control group age summary(nhefs.nmv[which(nhefs.nmv$qsmk==0), ]$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 25.0 33.0 42.0 42.8 51.0 72.0 summary(nhefs.nmv[which(nhefs.nmv$qsmk==1), ]$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 25.0 35.0 46.0 46.2 56.0 74.0 nhefs.nmv %&gt;% ggplot( aes(x=qsmk, y=age, fill=qsmk)) + geom_boxplot()+ scale_fill_viridis(discrete = TRUE, alpha=0.6) + geom_jitter(color=&quot;black&quot;, size=0.4, alpha=0.9) + theme_ipsum() + theme(legend.position=&quot;none&quot;, plot.title = element_text(size=11)) + ggtitle(&quot;Boxplot&quot;) + xlab(&quot;qsmk&quot;) wt71 summary(nhefs.nmv[which(nhefs.nmv$qsmk==0), ]$wt71) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 40.8 59.2 68.5 70.3 79.4 151.7 summary(nhefs.nmv[which(nhefs.nmv$qsmk==1), ]$wt71) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 39.6 60.7 71.2 72.4 81.1 137.0 nhefs.nmv %&gt;% ggplot( aes(x=qsmk, y=wt71, fill=qsmk)) + geom_boxplot()+ scale_fill_viridis(discrete = TRUE, alpha=0.6) + geom_jitter(color=&quot;black&quot;, size=0.4, alpha=0.9) + theme_ipsum() + theme(legend.position=&quot;none&quot;, plot.title = element_text(size=11)) + ggtitle(&quot;Boxplot&quot;) + xlab(&quot;qsmk&quot;) smokeintensity: number of cigarettes smoked per day in 1971 summary(nhefs.nmv[which(nhefs.nmv$qsmk==0), ]$smokeintensity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 15.0 20.0 21.2 30.0 60.0 summary(nhefs.nmv[which(nhefs.nmv$qsmk==1), ]$smokeintensity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 10.0 20.0 18.6 25.0 80.0 nhefs.nmv %&gt;% ggplot( aes(x=qsmk, y=smokeintensity, fill=qsmk)) + geom_boxplot()+ scale_fill_viridis(discrete = TRUE, alpha=0.6) + geom_jitter(color=&quot;black&quot;, size=0.4, alpha=0.9) + theme_ipsum() + theme(legend.position=&quot;none&quot;, plot.title = element_text(size=11)) + ggtitle(&quot;Boxplot&quot;) + xlab(&quot;qsmk&quot;) smokeyrs: years of smoking summary(nhefs.nmv[which(nhefs.nmv$qsmk==0), ]$smokeyrs) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 15.0 23.0 24.1 32.0 64.0 summary(nhefs.nmv[which(nhefs.nmv$qsmk==1), ]$smokeyrs) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 15 26 26 35 60 nhefs.nmv %&gt;% ggplot( aes(x=qsmk, y=smokeyrs, fill=qsmk)) + geom_boxplot()+ scale_fill_viridis(discrete = TRUE, alpha=0.6) + geom_jitter(color=&quot;black&quot;, size=0.4, alpha=0.9) + theme_ipsum() + theme(legend.position=&quot;none&quot;, plot.title = element_text(size=11)) + ggtitle(&quot;Boxplot&quot;) + xlab(&quot;qsmk&quot;) sex table(nhefs.nmv$qsmk, nhefs.nmv$sex) ## ## 0 1 ## 0 542 621 ## 1 220 183 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$sex), margin=1) # row-wise ## ## 0 1 ## 0 0.466 0.534 ## 1 0.546 0.454 nhefs.nmv$sex=as.factor(nhefs.nmv$sex) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = sex)) + geom_bar(position = &quot;dodge&quot;) race: 0 if white, 1 if black or other in 1971 table(nhefs.nmv$qsmk, nhefs.nmv$race) ## ## 0 1 ## 0 993 170 ## 1 367 36 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$race), margin=1) # row-wise ## ## 0 1 ## 0 0.8538 0.1462 ## 1 0.9107 0.0893 nhefs.nmv$race=as.factor(nhefs.nmv$race) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = race)) + geom_bar(position = &quot;dodge&quot;) education (AMOUNT OF EDUCATION BY 1971): 1 if 8TH GRADE OR LESS, 2 if HS DROPOUT, 3 if HS, 4 if COLLEGE DROPOUT, 5 if COLLEGE OR MORE table(nhefs.nmv$qsmk, nhefs.nmv$education) ## ## 1 2 3 4 5 ## 0 210 266 480 92 115 ## 1 81 74 157 29 62 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$education), margin=1) # row-wise ## ## 1 2 3 4 5 ## 0 0.1806 0.2287 0.4127 0.0791 0.0989 ## 1 0.2010 0.1836 0.3896 0.0720 0.1538 nhefs.nmv$education=as.factor(nhefs.nmv$education) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = education)) + geom_bar(position = &quot;dodge&quot;) exercise: 0 if much exercise,1 if moderate exercise,2 if little or no exercise table(nhefs.nmv$qsmk, nhefs.nmv$exercise) ## ## 0 1 2 ## 0 237 485 441 ## 1 63 176 164 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$exercise), margin=1) # row-wise ## ## 0 1 2 ## 0 0.204 0.417 0.379 ## 1 0.156 0.437 0.407 nhefs.nmv$exercise=as.factor(nhefs.nmv$exercise) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = exercise)) + geom_bar(position = &quot;dodge&quot;) active: 0 if very active, 1 if moderately active, 2 if inactive table(nhefs.nmv$qsmk, nhefs.nmv$active) ## ## 0 1 2 ## 0 532 527 104 ## 1 170 188 45 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$active), margin=1) # row-wise ## ## 0 1 2 ## 0 0.4574 0.4531 0.0894 ## 1 0.4218 0.4665 0.1117 nhefs.nmv$active=as.factor(nhefs.nmv$active) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = active)) + geom_bar(position = &quot;dodge&quot;) Estimating IP weights via modeling Step 1: Estimate \\(f(A|L)\\) by logistic regression and let \\(\\widehat{W}_i=\\frac{1}{\\widehat{P}(A|L)}\\) Step 2: Estimate the IP weighted mean \\(E\\left(\\frac{I(A=a)}{f(A|L)}Y\\right)\\) by Hajek estimator \\(\\frac{\\sum_{i}\\widehat{W}_iY_i}{\\sum_{i}\\widehat{W}_i}\\), where the sum is over all subjects with \\(A_i=a\\) ## run logistic regression logfit&lt;-glm( qsmk~sex+race+age+I(age^2)+education+smokeintensity+I(smokeintensity^2)+ smokeyrs+I(smokeyrs^2)+exercise+active+wt71+I(wt71^2), family=binomial(), data=nhefs.nmv ) summary(logfit) ## ## Call: ## glm(formula = qsmk ~ sex + race + age + I(age^2) + education + ## smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + ## exercise + active + wt71 + I(wt71^2), family = binomial(), ## data = nhefs.nmv) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.513 -0.791 -0.639 0.983 2.373 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.242519 1.380836 -1.62 0.10437 ## sex1 -0.527478 0.154050 -3.42 0.00062 *** ## race1 -0.839264 0.210067 -4.00 6.5e-05 *** ## age 0.121205 0.051266 2.36 0.01807 * ## I(age^2) -0.000825 0.000536 -1.54 0.12404 ## education2 -0.028776 0.198351 -0.15 0.88465 ## education3 0.086432 0.178085 0.49 0.62744 ## education4 0.063601 0.273211 0.23 0.81592 ## education5 0.475961 0.226224 2.10 0.03538 * ## smokeintensity -0.077270 0.015250 -5.07 4.0e-07 *** ## I(smokeintensity^2) 0.001045 0.000287 3.65 0.00027 *** ## smokeyrs -0.073597 0.027777 -2.65 0.00806 ** ## I(smokeyrs^2) 0.000844 0.000463 1.82 0.06840 . ## exercise1 0.354841 0.180135 1.97 0.04885 * ## exercise2 0.395704 0.187240 2.11 0.03457 * ## active1 0.031944 0.132937 0.24 0.81010 ## active2 0.176784 0.214972 0.82 0.41087 ## wt71 -0.015236 0.026316 -0.58 0.56262 ## I(wt71^2) 0.000135 0.000163 0.83 0.40737 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1786.1 on 1565 degrees of freedom ## Residual deviance: 1676.9 on 1547 degrees of freedom ## AIC: 1715 ## ## Number of Fisher Scoring iterations: 4 ## calculate estimated probability p.qsmk.obs=ifelse(nhefs.nmv$qsmk==0, 1-predict(logfit, type=&quot;response&quot;), predict(logfit, type=&quot;response&quot;) ## this predicts the conditional probability of treatment 1 ) nhefs.nmv$ipw=1/p.qsmk.obs summary(nhefs.nmv$ipw) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.05 1.23 1.37 2.00 1.99 16.70 ## run weighted least squares wls=geeglm(wt82_71~qsmk, data=nhefs.nmv, weights=ipw, id=seqn, corstr = &quot;independence&quot; ) summary(wls) ## ## Call: ## geeglm(formula = wt82_71 ~ qsmk, data = nhefs.nmv, weights = ipw, ## id = seqn, corstr = &quot;independence&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) 1.780 0.225 62.7 2.3e-15 *** ## qsmk1 3.441 0.525 42.9 5.9e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = independence ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 65.1 4.22 ## Number of clusters: 1566 Maximum cluster size: 1 ## build 95% confidence intervals for coefficients ## use the robust variance estimator, see more details in page 152 of the book theta=coef(wls) se=coef(summary(wls))[,2] ci.lower=theta-qnorm(0.975)*se ci.upper=theta+qnorm(0.975)*se cbind(theta, ci.lower, ci.upper) ## theta ci.lower ci.upper ## (Intercept) 1.78 1.34 2.22 ## qsmk1 3.44 2.41 4.47 ## estimate average causal effect ## A=1 y1=nhefs.nmv[nhefs.nmv$qsmk==1, ]$wt82_71 w1=nhefs.nmv[nhefs.nmv$qsmk==1, ]$ipw ## A=0 y0=nhefs.nmv[nhefs.nmv$qsmk==0, ]$wt82_71 w0=nhefs.nmv[nhefs.nmv$qsmk==0, ]$ipw ate=sum(y1*w1)/sum(w1)-sum(y0*w0)/sum(w0) ate ## [1] 3.44 ## association of sex and qsmk in the original population xtabs(~nhefs.nmv$sex+nhefs.nmv$qsmk) ## nhefs.nmv$qsmk ## nhefs.nmv$sex 0 1 ## 0 542 220 ## 1 621 183 ## no association between sex and qsmk in the pseudo-population xtabs(nhefs.nmv$ipw~nhefs.nmv$sex+nhefs.nmv$qsmk) ## nhefs.nmv$qsmk ## nhefs.nmv$sex 0 1 ## 0 764 764 ## 1 802 797 ## check for positivity (white women) xtabs(~nhefs.nmv$age[nhefs.nmv$race==0&amp;nhefs.nmv$sex==1]+ nhefs.nmv$qsmk[nhefs.nmv$race==0&amp;nhefs.nmv$sex==1]) ## nhefs.nmv$qsmk[nhefs.nmv$race == 0 &amp; nhefs.nmv$sex == 1] ## nhefs.nmv$age[nhefs.nmv$race == 0 &amp; nhefs.nmv$sex == 1] 0 1 ## 25 24 3 ## 26 14 5 ## 27 18 2 ## 28 20 5 ## 29 15 4 ## 30 14 5 ## 31 11 5 ## 32 14 7 ## 33 12 3 ## 34 22 5 ## 35 16 5 ## 36 13 3 ## 37 14 1 ## 38 6 2 ## 39 19 4 ## 40 10 4 ## 41 13 3 ## 42 16 3 ## 43 14 3 ## 44 9 4 ## 45 12 5 ## 46 19 4 ## 47 19 4 ## 48 19 4 ## 49 11 3 ## 50 18 4 ## 51 9 3 ## 52 11 3 ## 53 11 4 ## 54 17 9 ## 55 9 4 ## 56 8 7 ## 57 9 2 ## 58 8 4 ## 59 5 4 ## 60 5 4 ## 61 5 2 ## 62 6 5 ## 63 3 3 ## 64 7 1 ## 65 3 2 ## 66 4 0 ## 67 2 0 ## 69 6 2 ## 70 2 1 ## 71 0 1 ## 72 2 2 ## 74 0 1 Stablized IP weights ## estimate the denominator and numerator of the stablized IP weights denom.logfit&lt;-glm( qsmk~sex+race+age+I(age^2)+education+smokeintensity+I(smokeintensity^2)+ smokeyrs+I(smokeyrs^2)+exercise+active+wt71+I(wt71^2), family=binomial(), data=nhefs.nmv ) summary(denom.logfit) ## ## Call: ## glm(formula = qsmk ~ sex + race + age + I(age^2) + education + ## smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + ## exercise + active + wt71 + I(wt71^2), family = binomial(), ## data = nhefs.nmv) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.513 -0.791 -0.639 0.983 2.373 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.242519 1.380836 -1.62 0.10437 ## sex1 -0.527478 0.154050 -3.42 0.00062 *** ## race1 -0.839264 0.210067 -4.00 6.5e-05 *** ## age 0.121205 0.051266 2.36 0.01807 * ## I(age^2) -0.000825 0.000536 -1.54 0.12404 ## education2 -0.028776 0.198351 -0.15 0.88465 ## education3 0.086432 0.178085 0.49 0.62744 ## education4 0.063601 0.273211 0.23 0.81592 ## education5 0.475961 0.226224 2.10 0.03538 * ## smokeintensity -0.077270 0.015250 -5.07 4.0e-07 *** ## I(smokeintensity^2) 0.001045 0.000287 3.65 0.00027 *** ## smokeyrs -0.073597 0.027777 -2.65 0.00806 ** ## I(smokeyrs^2) 0.000844 0.000463 1.82 0.06840 . ## exercise1 0.354841 0.180135 1.97 0.04885 * ## exercise2 0.395704 0.187240 2.11 0.03457 * ## active1 0.031944 0.132937 0.24 0.81010 ## active2 0.176784 0.214972 0.82 0.41087 ## wt71 -0.015236 0.026316 -0.58 0.56262 ## I(wt71^2) 0.000135 0.000163 0.83 0.40737 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1786.1 on 1565 degrees of freedom ## Residual deviance: 1676.9 on 1547 degrees of freedom ## AIC: 1715 ## ## Number of Fisher Scoring iterations: 4 numer.logfit&lt;-glm(qsmk~1, family=binomial(), data=nhefs.nmv) summary(numer.logfit) ## ## Call: ## glm(formula = qsmk ~ 1, family = binomial(), data = nhefs.nmv) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.771 -0.771 -0.771 1.648 1.648 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.0598 0.0578 -18.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1786.1 on 1565 degrees of freedom ## Residual deviance: 1786.1 on 1565 degrees of freedom ## AIC: 1788 ## ## Number of Fisher Scoring iterations: 4 pd.qsmk=predict(denom.logfit, type=&quot;response&quot;) pn.qsmk=predict(numer.logfit, type=&quot;response&quot;) nhefs.nmv$sw=ifelse(nhefs.nmv$qsmk==0, ((1-pn.qsmk)/(1-pd.qsmk)), (pn.qsmk/pd.qsmk)) ## weighted least squares with the stabilized IP weights wls.sw=geeglm(wt82_71~qsmk, data=nhefs.nmv, weights=sw, id=seqn, corstr=&quot;independence&quot; ) summary(wls.sw) ## ## Call: ## geeglm(formula = wt82_71 ~ qsmk, data = nhefs.nmv, weights = sw, ## id = seqn, corstr = &quot;independence&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) 1.780 0.225 62.7 2.3e-15 *** ## qsmk1 3.441 0.525 42.9 5.9e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = independence ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 60.7 3.71 ## Number of clusters: 1566 Maximum cluster size: 1 theta=coef(wls.sw) se=coef(summary(wls.sw))[,2] ci.lower=theta-qnorm(0.975)*se ci.upper=theta+qnorm(0.975)*se cbind(theta, ci.lower, ci.upper) ## theta ci.lower ci.upper ## (Intercept) 1.78 1.34 2.22 ## qsmk1 3.44 2.41 4.47 ## no association between sex and qsmk in the pseudo population xtabs(nhefs.nmv$sw~nhefs.nmv$sex+nhefs.nmv$qsmk) ## nhefs.nmv$qsmk ## nhefs.nmv$sex 0 1 ## 0 567 197 ## 1 595 205 ## estimate average causal effect ## A=1 y1=nhefs.nmv[nhefs.nmv$qsmk==1, ]$wt82_71 w1=nhefs.nmv[nhefs.nmv$qsmk==1, ]$sw ## A=0 y0=nhefs.nmv[nhefs.nmv$qsmk==0, ]$wt82_71 w0=nhefs.nmv[nhefs.nmv$qsmk==0, ]$sw ate=sum(y1*w1)/sum(w1)-sum(y0*w0)/sum(w0) ate ## [1] 3.44 Marginal structural models Continuous outcome Estimating stabilized weights Approaches in the book: we need to estimate the stabilized weights \\(SW^A=f(A)/f(A|L)\\). For a continuous treatment \\(A\\), \\(f(A|L)\\) is a probability density function. We assume that the density \\(f(A|L)\\) is normal with mean \\(\\mu_L=E(A|L)\\) and constant variance \\(\\sigma^2\\). Then we use a linear regression model to estimate the mean \\(E(A|L)\\) and variance of residuals \\(\\sigma^2\\) for all combinations of values of \\(L\\). We also assume the density \\(f(A)\\) is normal. ## analysis restricted to subjects reporting &lt;=25 cig/day at baseline nhefs.nmv.s=subset(nhefs.nmv, smokeintensity&lt;=25) ## estimating denominator in ip weights den.fit.obj=lm( smkintensity82_71~sex+race+age+I(age^2)+ education+smokeintensity+I(smokeintensity^2)+ smokeyrs+I(smokeyrs^2)+exercise+active+wt71+I(wt71^2), data=nhefs.nmv.s ) p.den=predict(den.fit.obj, type=&quot;response&quot;) dens.den=dnorm(nhefs.nmv.s$smkintensity82_71, mean=p.den, sd=summary(den.fit.obj)$sigma) ## estimating numerator in ip weights num.fit.obj=lm(smkintensity82_71~1, data=nhefs.nmv.s) p.num=predict(num.fit.obj, type=&quot;response&quot;) dens.num=dnorm(nhefs.nmv.s$smkintensity82_71, p.num, summary(num.fit.obj)$sigma) nhefs.nmv.s$sw.a=dens.num/dens.den summary(nhefs.nmv.s$sw.a) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.19 0.89 0.97 1.00 1.05 5.10 msm.sw.cont=geeglm(wt82_71~smkintensity82_71+I(smkintensity82_71*smkintensity82_71), data=nhefs.nmv.s, weights=sw.a, id=seqn, corstr=&quot;independence&quot;) summary(msm.sw.cont) ## ## Call: ## geeglm(formula = wt82_71 ~ smkintensity82_71 + I(smkintensity82_71 * ## smkintensity82_71), data = nhefs.nmv.s, weights = sw.a, id = seqn, ## corstr = &quot;independence&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) 2.00452 0.29512 46.13 1.1e-11 *** ## smkintensity82_71 -0.10899 0.03154 11.94 0.00055 *** ## I(smkintensity82_71 * smkintensity82_71) 0.00269 0.00242 1.24 0.26489 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = independence ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 60.5 4.5 ## Number of clusters: 1162 Maximum cluster size: 1 beta=coef(msm.sw.cont) SE=coef(summary(msm.sw.cont))[,2] lcl=beta-qnorm(0.975)*SE ucl=beta+qnorm(0.975)*SE cbind(beta, lcl, ucl) ## beta lcl ucl ## (Intercept) 2.00452 1.42610 2.58295 ## smkintensity82_71 -0.10899 -0.17080 -0.04718 ## I(smkintensity82_71 * smkintensity82_71) 0.00269 -0.00204 0.00743 Program 12.5: Dichotomous outcome Goal: estimate the causal effect of quitting smoking \\(A\\) (1: yes, 0: no) on the risk of death \\(D\\) (1: yes, 0: no) by 1992 Marginal structural logistic model: \\(\\text{logit} P(D^{a}=1)=\\alpha_0+\\alpha_1 a\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
