[["index.html", "R code for causal inference book Preface", " R code for causal inference book Duzhe Wang 2021-01-12 Preface The R code is based on the code by Joy Shi and Sean McGrath given here, and the rendered version by Tom Palmer given here. "],["why-model.html", "11. Why model? Some concepts and points Program 11.1 Program 11.2 Program 11.3", " 11. Why model? Some concepts and points Estimand Estimator We can not always let the data “speak for themselves” to obtain a meaningful estimate. Rather, we often need to supplement the data with a model. What is a model? A model is defined by an a priori restriction on the joint distribution of the data. When using a parametric model, the inferences are correct only if the restrictions encoded in the model are correct, i.e., if the model is correctly specified. Thus model-based causal inference relies on conditions of no model misspecification. Fisher consistency: An estimator of a population quantity that, when calculated using the entire population rather than a sample, yields the true value of the population parameter. Bias-variance trade-off Program 11.1 Sample averages by treatment level Data from Figures 11.1 and 11.2 A&lt;-c(rep(1, 8), rep(0, 8)) Y &lt;- c(200, 150, 220, 110, 50, 180, 90, 170, 170, 30, 70, 110, 80, 50, 10, 20) plot(A, Y, pch=16) mean(Y[A == 0]) ## [1] 67.5 mean(Y[A == 1]) ## [1] 146 A2&lt;-c(rep(1,4), rep(2, 4), rep(3, 4), rep(4,4)) Y2 &lt;- c(110, 80, 50, 40, 170, 30, 70, 50, 110, 50, 180, 130, 200, 150, 220, 210) plot(A2, Y2, pch=16) mean(Y2[A2 == 1]) ## [1] 70 mean(Y2[A2 == 2]) ## [1] 80 mean(Y2[A2 == 3]) ## [1] 118 mean(Y2[A2 == 4]) ## [1] 195 Program 11.2 2-parameter linear model Data from Figures 11.3 and 11.1 A3 &lt;-c(3, 11, 17, 23, 29, 37, 41, 53, 67, 79, 83, 97, 60, 71, 15, 45) Y3 &lt;-c(21, 54, 33, 101, 85, 65, 157, 120, 111, 200, 140, 220, 230, 217, 11, 190) plot(Y3 ~ A3, pch=16) summary(glm(Y3 ~ A3)) ## ## Call: ## glm(formula = Y3 ~ A3) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -61.93 -30.56 -5.74 30.65 77.22 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.55 21.33 1.15 0.2691 ## A3 2.14 0.40 5.35 0.0001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1944) ## ## Null deviance: 82800 on 15 degrees of freedom ## Residual deviance: 27218 on 14 degrees of freedom ## AIC: 170.4 ## ## Number of Fisher Scoring iterations: 2 predict(glm(Y3 ~ A3), data.frame(A3 = 90)) ## 1 ## 217 summary(glm(Y ~ A)) ## ## Call: ## glm(formula = Y ~ A) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -96.25 -40.00 3.12 35.94 102.50 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 67.5 19.7 3.42 0.0041 ** ## A 78.8 27.9 2.82 0.0135 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 3110) ## ## Null deviance: 68344 on 15 degrees of freedom ## Residual deviance: 43538 on 14 degrees of freedom ## AIC: 177.9 ## ## Number of Fisher Scoring iterations: 2 Program 11.3 3-parameter linear model: \\(E(Y|A)=\\theta_0+\\theta_{1}A+\\theta_{2}A^2\\), where \\(A^2=A\\times A\\) Data from Figure 11.3 Asq &lt;- A3 * A3 mod3 &lt;- glm(Y3 ~ A3 + Asq) summary(mod3) ## ## Call: ## glm(formula = Y3 ~ A3 + Asq) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -65.3 -34.4 13.2 26.1 64.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.4069 31.7478 -0.23 0.819 ## A3 4.1072 1.5309 2.68 0.019 * ## Asq -0.0204 0.0153 -1.33 0.206 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1843) ## ## Null deviance: 82800 on 15 degrees of freedom ## Residual deviance: 23955 on 13 degrees of freedom ## AIC: 170.4 ## ## Number of Fisher Scoring iterations: 2 predict(mod3, data.frame(cbind(A3 = 90, Asq = 8100))) ## 1 ## 197 "],["ip-weighting-and-marginal-structural-models.html", "12. IP weighting and marginal structural models Estimating IP weights via modeling Stablized IP weights Marginal structural models Real data analysis", " 12. IP weighting and marginal structural models Estimating IP weights via modeling Definition An individual’s IP weight depends on her values of treatment \\(A\\) and covariate \\(L\\). Since the denominator of the weight for each individual is the conditional density evaluated at the individual’s own values of \\(A\\) and \\(L\\), it can be expressed as the conditional density evaluated at the random arguments \\(A\\) and \\(L\\), that is, as \\(f(A|L)\\). Therefore, we write the IP weights as \\(1/f(A|L)\\). Potential outcome mean for treatment level \\(A=a\\): \\(E(Y^{a})\\) Standardized mean for treatment level \\(A=a\\): \\(\\sum_{l}E(Y|A=a, L=l)P(L=l)\\) IP weighted mean of \\(Y\\) for treatment level \\(A=a\\): \\(E\\left(\\frac{I(A=a)Y}{f(A|L)}\\right)\\) Equivalence of IP weighting and standardization Under the positivity assumption, we have \\[\\begin{equation*} \\begin{split} E\\left(\\frac{I(A=a)Y}{f(A|L)}\\right)=E_{A, L}\\left( E_{Y}(\\frac{I(A=a)}{f(A|L)}Y|A, L)\\right)=E_{A, L}\\left( E_{Y}(Y|A, L)\\frac{I(A=a)}{f(A|L)}\\right) \\\\ = E_{L}\\left(E_{A}\\left( E_{Y}(Y|A, L)\\frac{I(A=a)}{f(A|L)}|L\\right)\\right)=E_{L}\\left( E_{Y}(Y|A=a, L)|L\\right) \\\\ =\\sum_{l}E(Y|A=a, L=l)P(L=l) \\end{split} \\end{equation*}\\] Equivalence of potential outcome mean, standardized mean and IP weighted mean First, we show \\(E(Y^a)=\\sum_{l}E(Y|A=a, L=l)P(L=l)\\) under conditional exchangeability, positivity, and consistency. \\[\\begin{equation*} \\begin{split} E(Y^{a})=\\sum_{l}E(Y^a|L=l)P(L=l) \\\\ =\\sum_{l}E(Y^a|A=a, L=l)P(L=l)=\\sum_{l}E(Y|A=a, L=l)P(L=l), \\end{split} \\end{equation*}\\] where the second equality is by conditional exchangeability and positivity, and the third by consistency. Second, we show \\(E(Y^a)=E\\left( \\frac{I(A=a)}{f(A|L)}Y \\right)\\) under positivity, conditional exchangeability, and consistency. \\[\\begin{equation*} \\begin{split} E\\left(\\frac{I(A=a)}{f(A|L)}Y\\right)= E\\left( \\frac{I(A=a)}{f(A|L)}Y^a \\right)=E\\left( E\\left(\\frac{I(A=a)}{f(A|L)}Y^{a}|L\\right) \\right) \\\\ = E\\left(E\\left(\\frac{I(A=a)}{f(A|L)}|L\\right)E\\left(Y^a|L\\right) \\right)=E\\left(1\\times E\\left(Y^{a}|L\\right) \\right)=E(Y^a), \\end{split} \\end{equation*}\\] where the first equality is by consistency, the third by conditional exchangeability. Furthermore, we need positivity to well define the IP weights. What does IP weighting mean? IP weighting creates a pseudo-population in which the arrow from the covariates \\(L\\) to the treatment \\(A\\) is removed (ther is no association between covariates \\(L\\) and the treatment \\(A\\)). The pseudo-population has the following two properties: - \\(A\\) and \\(L\\) are statistically independent - the mean \\(E_{ps}(Y|A=a)\\) in the pseudo-population equals the standardized mean \\(\\sum_{l}E(Y|A=a, L=l)P(L=l)\\) in the actual population under positivity Horvitz-Thompson estimator and Hajek estimator The IP weighted mean can be consistently estimated by Horvitz-Thompson estimator \\(\\widehat{E}(\\frac{I(A=a)}{f(A|L)}Y)\\) or the Hajek estimator \\(\\frac{\\widehat{E}(\\frac{I(A=a)}{f(A|L)}Y)}{\\widehat{E}(\\frac{I(A=a)}{f(A|L)})}\\), which is an unbiased estimator of \\(\\frac{E(\\frac{I(A=a)}{f(A|L)}Y)}{E(\\frac{I(A=a)}{f(A|L)})}\\). Note under positivity, \\(\\frac{E(\\frac{I(A=a)}{f(A|L)}Y)}{E(\\frac{I(A=a)}{f(A|L)})}=E(\\frac{I(A=a)}{f(A|L)}Y)\\). In practice, the Hajek estimator is preferred because it is guaranteed to lie between 0 and 1 for dichotomous \\(Y\\). Stablized IP weights Several facts from mathematics: under positivity, conditional exchangeability, and consistency, \\[\\begin{equation*} \\frac{E(\\frac{pI(A=a)}{f(A|L)}Y)}{E(\\frac{pI(A=a)}{f(A|L)})}=E(Y^a), \\end{equation*}\\] and \\[\\begin{equation*} \\frac{E(\\frac{g(A)I(A=a)}{f(A|L)}Y)}{E(\\frac{g(A)I(A=a)}{f(A|L)})}=E(Y^a), \\end{equation*}\\] where g(A) is any function of \\(A\\) that is not a function of \\(L\\). Usually we take \\(g(A)=f(A)\\). The IP weights \\(W^{A}=1/f(A|L)\\) are referred to as nonstabilized weights, and the IP weights \\(SW^{A}=f(A)/f(A|L)\\) are referred to as stabilized weights. The stablizing factor \\(f(A)\\) in the numerator is responsible for the narrower range of the \\(f(A)/f(A|L)\\) weights. More on IPW: http://www.rebeccabarter.com/blog/2017-07-05-ip-weighting/ Marginal structural models Examples of marginal structural mean models: \\(E(Y^a)=\\beta_0+\\beta_1a\\) or \\(E(Y^a)=\\beta_0+\\beta_1a+\\beta_2a^2\\) Real data analysis Background NHEFS data (National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study) Goal: estimate the effect of smoking cessation on weight gain \\(A=1\\): if cigarette smokers reported having quit smoking before the follow-up visit \\(Y\\): the body weight at the follow-up visit minus the body weight at the baseline visit \\(E(Y^{a=1})\\): mean weight gain that would have been observed if all individuals in the population had quit smoking before the follow-up visit \\(E(Y^{a=0})\\): mean weight gain that would have been observed if all individuals in the population had not quit smoking Average causal effect: \\(E(Y^{a=1})-E(Y^{a=0})\\) library(here) # for path library(readxl) # for reafing excel files library(tidyverse) library(hrbrthemes) library(viridis) library(geepack) # for GEE # copy from https://www.r-graph-gallery.com/89-box-and-scatter-plot-with-ggplot2.html Input dataset nhefs=read_excel(here(&quot;data&quot;, &quot;NHEFS.xls&quot;)) head(nhefs) ## # A tibble: 6 x 64 ## seqn qsmk death yrdth modth dadth sbp dbp sex age race income marital school education ht ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 233 0 0 NA NA NA 175 96 0 42 1 19 2 7 1 174. ## 2 235 0 0 NA NA NA 123 80 0 36 0 18 2 9 2 159. ## 3 244 0 0 NA NA NA 115 75 1 56 1 15 3 11 2 168. ## 4 245 0 1 85 2 14 148 78 0 68 1 15 3 5 1 170. ## 5 252 0 0 NA NA NA 118 77 0 40 0 18 2 11 2 182. ## 6 257 0 0 NA NA NA 141 83 1 43 1 11 4 9 2 162. ## # … with 48 more variables: wt71 &lt;dbl&gt;, wt82 &lt;dbl&gt;, wt82_71 &lt;dbl&gt;, birthplace &lt;dbl&gt;, smokeintensity &lt;dbl&gt;, ## # smkintensity82_71 &lt;dbl&gt;, smokeyrs &lt;dbl&gt;, asthma &lt;dbl&gt;, bronch &lt;dbl&gt;, tb &lt;dbl&gt;, hf &lt;dbl&gt;, hbp &lt;dbl&gt;, ## # pepticulcer &lt;dbl&gt;, colitis &lt;dbl&gt;, hepatitis &lt;dbl&gt;, chroniccough &lt;dbl&gt;, hayfever &lt;dbl&gt;, diabetes &lt;dbl&gt;, ## # polio &lt;dbl&gt;, tumor &lt;dbl&gt;, nervousbreak &lt;dbl&gt;, alcoholpy &lt;dbl&gt;, alcoholfreq &lt;dbl&gt;, alcoholtype &lt;dbl&gt;, ## # alcoholhowmuch &lt;dbl&gt;, pica &lt;dbl&gt;, headache &lt;dbl&gt;, otherpain &lt;dbl&gt;, weakheart &lt;dbl&gt;, allergies &lt;dbl&gt;, ## # nerves &lt;dbl&gt;, lackpep &lt;dbl&gt;, hbpmed &lt;dbl&gt;, boweltrouble &lt;dbl&gt;, wtloss &lt;dbl&gt;, infection &lt;dbl&gt;, ## # active &lt;dbl&gt;, exercise &lt;dbl&gt;, birthcontrol &lt;dbl&gt;, pregnancies &lt;dbl&gt;, cholesterol &lt;dbl&gt;, hightax82 &lt;dbl&gt;, ## # price71 &lt;dbl&gt;, price82 &lt;dbl&gt;, tax71 &lt;dbl&gt;, tax82 &lt;dbl&gt;, price71_82 &lt;dbl&gt;, tax71_82 &lt;dbl&gt; Ignore subjects with missing values for weight in 1982 wt82: weight in 1982 qsmk: quit smoking between 1st questionnaire and 1982 (Yes:1; No:0) nhefs.nmv=nhefs[which(!is.na(nhefs$wt82)), ] nhefs.nmv$qsmk=as.factor(nhefs.nmv$qsmk) Compare the treatment group and the control group age summary(nhefs.nmv[which(nhefs.nmv$qsmk==0), ]$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 25.0 33.0 42.0 42.8 51.0 72.0 summary(nhefs.nmv[which(nhefs.nmv$qsmk==1), ]$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 25.0 35.0 46.0 46.2 56.0 74.0 nhefs.nmv %&gt;% ggplot( aes(x=qsmk, y=age, fill=qsmk)) + geom_boxplot()+ scale_fill_viridis(discrete = TRUE, alpha=0.6) + geom_jitter(color=&quot;black&quot;, size=0.4, alpha=0.9) + theme_ipsum() + theme(legend.position=&quot;none&quot;, plot.title = element_text(size=11)) + ggtitle(&quot;Boxplot&quot;) + xlab(&quot;qsmk&quot;) wt71 summary(nhefs.nmv[which(nhefs.nmv$qsmk==0), ]$wt71) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 40.8 59.2 68.5 70.3 79.4 151.7 summary(nhefs.nmv[which(nhefs.nmv$qsmk==1), ]$wt71) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 39.6 60.7 71.2 72.4 81.1 137.0 nhefs.nmv %&gt;% ggplot( aes(x=qsmk, y=wt71, fill=qsmk)) + geom_boxplot()+ scale_fill_viridis(discrete = TRUE, alpha=0.6) + geom_jitter(color=&quot;black&quot;, size=0.4, alpha=0.9) + theme_ipsum() + theme(legend.position=&quot;none&quot;, plot.title = element_text(size=11)) + ggtitle(&quot;Boxplot&quot;) + xlab(&quot;qsmk&quot;) smokeintensity: number of cigarettes smoked per day in 1971 summary(nhefs.nmv[which(nhefs.nmv$qsmk==0), ]$smokeintensity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 15.0 20.0 21.2 30.0 60.0 summary(nhefs.nmv[which(nhefs.nmv$qsmk==1), ]$smokeintensity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 10.0 20.0 18.6 25.0 80.0 nhefs.nmv %&gt;% ggplot( aes(x=qsmk, y=smokeintensity, fill=qsmk)) + geom_boxplot()+ scale_fill_viridis(discrete = TRUE, alpha=0.6) + geom_jitter(color=&quot;black&quot;, size=0.4, alpha=0.9) + theme_ipsum() + theme(legend.position=&quot;none&quot;, plot.title = element_text(size=11)) + ggtitle(&quot;Boxplot&quot;) + xlab(&quot;qsmk&quot;) smokeyrs: years of smoking summary(nhefs.nmv[which(nhefs.nmv$qsmk==0), ]$smokeyrs) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 15.0 23.0 24.1 32.0 64.0 summary(nhefs.nmv[which(nhefs.nmv$qsmk==1), ]$smokeyrs) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 15 26 26 35 60 nhefs.nmv %&gt;% ggplot( aes(x=qsmk, y=smokeyrs, fill=qsmk)) + geom_boxplot()+ scale_fill_viridis(discrete = TRUE, alpha=0.6) + geom_jitter(color=&quot;black&quot;, size=0.4, alpha=0.9) + theme_ipsum() + theme(legend.position=&quot;none&quot;, plot.title = element_text(size=11)) + ggtitle(&quot;Boxplot&quot;) + xlab(&quot;qsmk&quot;) sex table(nhefs.nmv$qsmk, nhefs.nmv$sex) ## ## 0 1 ## 0 542 621 ## 1 220 183 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$sex), margin=1) # row-wise ## ## 0 1 ## 0 0.466 0.534 ## 1 0.546 0.454 nhefs.nmv$sex=as.factor(nhefs.nmv$sex) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = sex)) + geom_bar(position = &quot;dodge&quot;) race: 0 if white, 1 if black or other in 1971 table(nhefs.nmv$qsmk, nhefs.nmv$race) ## ## 0 1 ## 0 993 170 ## 1 367 36 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$race), margin=1) # row-wise ## ## 0 1 ## 0 0.8538 0.1462 ## 1 0.9107 0.0893 nhefs.nmv$race=as.factor(nhefs.nmv$race) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = race)) + geom_bar(position = &quot;dodge&quot;) education (AMOUNT OF EDUCATION BY 1971): 1 if 8TH GRADE OR LESS, 2 if HS DROPOUT, 3 if HS, 4 if COLLEGE DROPOUT, 5 if COLLEGE OR MORE table(nhefs.nmv$qsmk, nhefs.nmv$education) ## ## 1 2 3 4 5 ## 0 210 266 480 92 115 ## 1 81 74 157 29 62 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$education), margin=1) # row-wise ## ## 1 2 3 4 5 ## 0 0.1806 0.2287 0.4127 0.0791 0.0989 ## 1 0.2010 0.1836 0.3896 0.0720 0.1538 nhefs.nmv$education=as.factor(nhefs.nmv$education) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = education)) + geom_bar(position = &quot;dodge&quot;) exercise: 0 if much exercise,1 if moderate exercise,2 if little or no exercise table(nhefs.nmv$qsmk, nhefs.nmv$exercise) ## ## 0 1 2 ## 0 237 485 441 ## 1 63 176 164 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$exercise), margin=1) # row-wise ## ## 0 1 2 ## 0 0.204 0.417 0.379 ## 1 0.156 0.437 0.407 nhefs.nmv$exercise=as.factor(nhefs.nmv$exercise) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = exercise)) + geom_bar(position = &quot;dodge&quot;) active: 0 if very active, 1 if moderately active, 2 if inactive table(nhefs.nmv$qsmk, nhefs.nmv$active) ## ## 0 1 2 ## 0 532 527 104 ## 1 170 188 45 prop.table(table(nhefs.nmv$qsmk, nhefs.nmv$active), margin=1) # row-wise ## ## 0 1 2 ## 0 0.4574 0.4531 0.0894 ## 1 0.4218 0.4665 0.1117 nhefs.nmv$active=as.factor(nhefs.nmv$active) nhefs.nmv%&gt;%ggplot(aes(x = qsmk, fill = active)) + geom_bar(position = &quot;dodge&quot;) Estimating IP weights via modeling Step 1: Estimate \\(f(A|L)\\) by logistic regression and let \\(\\widehat{W}_i=\\frac{1}{\\widehat{P}(A|L)}\\) Step 2: Estimate the IP weighted mean \\(E\\left(\\frac{I(A=a)}{f(A|L)}Y\\right)\\) by Hajek estimator \\(\\frac{\\sum_{i}\\widehat{W}_iY_i}{\\sum_{i}\\widehat{W}_i}\\), where the sum is over all subjects with \\(A_i=a\\) ## run logistic regression logfit&lt;-glm( qsmk~sex+race+age+I(age^2)+education+smokeintensity+I(smokeintensity^2)+ smokeyrs+I(smokeyrs^2)+exercise+active+wt71+I(wt71^2), family=binomial(), data=nhefs.nmv ) summary(logfit) ## ## Call: ## glm(formula = qsmk ~ sex + race + age + I(age^2) + education + ## smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + ## exercise + active + wt71 + I(wt71^2), family = binomial(), ## data = nhefs.nmv) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.513 -0.791 -0.639 0.983 2.373 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.242519 1.380836 -1.62 0.10437 ## sex1 -0.527478 0.154050 -3.42 0.00062 *** ## race1 -0.839264 0.210067 -4.00 6.5e-05 *** ## age 0.121205 0.051266 2.36 0.01807 * ## I(age^2) -0.000825 0.000536 -1.54 0.12404 ## education2 -0.028776 0.198351 -0.15 0.88465 ## education3 0.086432 0.178085 0.49 0.62744 ## education4 0.063601 0.273211 0.23 0.81592 ## education5 0.475961 0.226224 2.10 0.03538 * ## smokeintensity -0.077270 0.015250 -5.07 4.0e-07 *** ## I(smokeintensity^2) 0.001045 0.000287 3.65 0.00027 *** ## smokeyrs -0.073597 0.027777 -2.65 0.00806 ** ## I(smokeyrs^2) 0.000844 0.000463 1.82 0.06840 . ## exercise1 0.354841 0.180135 1.97 0.04885 * ## exercise2 0.395704 0.187240 2.11 0.03457 * ## active1 0.031944 0.132937 0.24 0.81010 ## active2 0.176784 0.214972 0.82 0.41087 ## wt71 -0.015236 0.026316 -0.58 0.56262 ## I(wt71^2) 0.000135 0.000163 0.83 0.40737 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1786.1 on 1565 degrees of freedom ## Residual deviance: 1676.9 on 1547 degrees of freedom ## AIC: 1715 ## ## Number of Fisher Scoring iterations: 4 ## calculate estimated probability p.qsmk.obs=ifelse(nhefs.nmv$qsmk==0, 1-predict(logfit, type=&quot;response&quot;), predict(logfit, type=&quot;response&quot;) ## this predicts the conditional probability of treatment 1 ) nhefs.nmv$ipw=1/p.qsmk.obs summary(nhefs.nmv$ipw) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.05 1.23 1.37 2.00 1.99 16.70 ## run weighted least squares wls=geeglm(wt82_71~qsmk, data=nhefs.nmv, weights=ipw, id=seqn, corstr = &quot;independence&quot; ) summary(wls) ## ## Call: ## geeglm(formula = wt82_71 ~ qsmk, data = nhefs.nmv, weights = ipw, ## id = seqn, corstr = &quot;independence&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) 1.780 0.225 62.7 2.3e-15 *** ## qsmk1 3.441 0.525 42.9 5.9e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = independence ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 65.1 4.22 ## Number of clusters: 1566 Maximum cluster size: 1 ## build 95% confidence intervals for coefficients ## use the robust variance estimator, see more details in page 152 of the book theta=coef(wls) se=coef(summary(wls))[,2] ci.lower=theta-qnorm(0.975)*se ci.upper=theta+qnorm(0.975)*se cbind(theta, ci.lower, ci.upper) ## theta ci.lower ci.upper ## (Intercept) 1.78 1.34 2.22 ## qsmk1 3.44 2.41 4.47 ## estimate average causal effect ## A=1 y1=nhefs.nmv[nhefs.nmv$qsmk==1, ]$wt82_71 w1=nhefs.nmv[nhefs.nmv$qsmk==1, ]$ipw ## A=0 y0=nhefs.nmv[nhefs.nmv$qsmk==0, ]$wt82_71 w0=nhefs.nmv[nhefs.nmv$qsmk==0, ]$ipw ate=sum(y1*w1)/sum(w1)-sum(y0*w0)/sum(w0) ate ## [1] 3.44 ## association of sex and qsmk in the original population xtabs(~nhefs.nmv$sex+nhefs.nmv$qsmk) ## nhefs.nmv$qsmk ## nhefs.nmv$sex 0 1 ## 0 542 220 ## 1 621 183 ## no association between sex and qsmk in the pseudo-population xtabs(nhefs.nmv$ipw~nhefs.nmv$sex+nhefs.nmv$qsmk) ## nhefs.nmv$qsmk ## nhefs.nmv$sex 0 1 ## 0 764 764 ## 1 802 797 ## check for positivity (white women) xtabs(~nhefs.nmv$age[nhefs.nmv$race==0&amp;nhefs.nmv$sex==1]+ nhefs.nmv$qsmk[nhefs.nmv$race==0&amp;nhefs.nmv$sex==1]) ## nhefs.nmv$qsmk[nhefs.nmv$race == 0 &amp; nhefs.nmv$sex == 1] ## nhefs.nmv$age[nhefs.nmv$race == 0 &amp; nhefs.nmv$sex == 1] 0 1 ## 25 24 3 ## 26 14 5 ## 27 18 2 ## 28 20 5 ## 29 15 4 ## 30 14 5 ## 31 11 5 ## 32 14 7 ## 33 12 3 ## 34 22 5 ## 35 16 5 ## 36 13 3 ## 37 14 1 ## 38 6 2 ## 39 19 4 ## 40 10 4 ## 41 13 3 ## 42 16 3 ## 43 14 3 ## 44 9 4 ## 45 12 5 ## 46 19 4 ## 47 19 4 ## 48 19 4 ## 49 11 3 ## 50 18 4 ## 51 9 3 ## 52 11 3 ## 53 11 4 ## 54 17 9 ## 55 9 4 ## 56 8 7 ## 57 9 2 ## 58 8 4 ## 59 5 4 ## 60 5 4 ## 61 5 2 ## 62 6 5 ## 63 3 3 ## 64 7 1 ## 65 3 2 ## 66 4 0 ## 67 2 0 ## 69 6 2 ## 70 2 1 ## 71 0 1 ## 72 2 2 ## 74 0 1 Stablized IP weights ## estimate the denominator and numerator of the stablized IP weights denom.logfit&lt;-glm( qsmk~sex+race+age+I(age^2)+education+smokeintensity+I(smokeintensity^2)+ smokeyrs+I(smokeyrs^2)+exercise+active+wt71+I(wt71^2), family=binomial(), data=nhefs.nmv ) summary(denom.logfit) ## ## Call: ## glm(formula = qsmk ~ sex + race + age + I(age^2) + education + ## smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + ## exercise + active + wt71 + I(wt71^2), family = binomial(), ## data = nhefs.nmv) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.513 -0.791 -0.639 0.983 2.373 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.242519 1.380836 -1.62 0.10437 ## sex1 -0.527478 0.154050 -3.42 0.00062 *** ## race1 -0.839264 0.210067 -4.00 6.5e-05 *** ## age 0.121205 0.051266 2.36 0.01807 * ## I(age^2) -0.000825 0.000536 -1.54 0.12404 ## education2 -0.028776 0.198351 -0.15 0.88465 ## education3 0.086432 0.178085 0.49 0.62744 ## education4 0.063601 0.273211 0.23 0.81592 ## education5 0.475961 0.226224 2.10 0.03538 * ## smokeintensity -0.077270 0.015250 -5.07 4.0e-07 *** ## I(smokeintensity^2) 0.001045 0.000287 3.65 0.00027 *** ## smokeyrs -0.073597 0.027777 -2.65 0.00806 ** ## I(smokeyrs^2) 0.000844 0.000463 1.82 0.06840 . ## exercise1 0.354841 0.180135 1.97 0.04885 * ## exercise2 0.395704 0.187240 2.11 0.03457 * ## active1 0.031944 0.132937 0.24 0.81010 ## active2 0.176784 0.214972 0.82 0.41087 ## wt71 -0.015236 0.026316 -0.58 0.56262 ## I(wt71^2) 0.000135 0.000163 0.83 0.40737 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1786.1 on 1565 degrees of freedom ## Residual deviance: 1676.9 on 1547 degrees of freedom ## AIC: 1715 ## ## Number of Fisher Scoring iterations: 4 numer.logfit&lt;-glm(qsmk~1, family=binomial(), data=nhefs.nmv) summary(numer.logfit) ## ## Call: ## glm(formula = qsmk ~ 1, family = binomial(), data = nhefs.nmv) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.771 -0.771 -0.771 1.648 1.648 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.0598 0.0578 -18.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1786.1 on 1565 degrees of freedom ## Residual deviance: 1786.1 on 1565 degrees of freedom ## AIC: 1788 ## ## Number of Fisher Scoring iterations: 4 pd.qsmk=predict(denom.logfit, type=&quot;response&quot;) pn.qsmk=predict(numer.logfit, type=&quot;response&quot;) nhefs.nmv$sw=ifelse(nhefs.nmv$qsmk==0, ((1-pn.qsmk)/(1-pd.qsmk)), (pn.qsmk/pd.qsmk)) ## weighted least squares with the stabilized IP weights wls.sw=geeglm(wt82_71~qsmk, data=nhefs.nmv, weights=sw, id=seqn, corstr=&quot;independence&quot; ) summary(wls.sw) ## ## Call: ## geeglm(formula = wt82_71 ~ qsmk, data = nhefs.nmv, weights = sw, ## id = seqn, corstr = &quot;independence&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) 1.780 0.225 62.7 2.3e-15 *** ## qsmk1 3.441 0.525 42.9 5.9e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = independence ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 60.7 3.71 ## Number of clusters: 1566 Maximum cluster size: 1 theta=coef(wls.sw) se=coef(summary(wls.sw))[,2] ci.lower=theta-qnorm(0.975)*se ci.upper=theta+qnorm(0.975)*se cbind(theta, ci.lower, ci.upper) ## theta ci.lower ci.upper ## (Intercept) 1.78 1.34 2.22 ## qsmk1 3.44 2.41 4.47 ## no association between sex and qsmk in the pseudo population xtabs(nhefs.nmv$sw~nhefs.nmv$sex+nhefs.nmv$qsmk) ## nhefs.nmv$qsmk ## nhefs.nmv$sex 0 1 ## 0 567 197 ## 1 595 205 ## estimate average causal effect ## A=1 y1=nhefs.nmv[nhefs.nmv$qsmk==1, ]$wt82_71 w1=nhefs.nmv[nhefs.nmv$qsmk==1, ]$sw ## A=0 y0=nhefs.nmv[nhefs.nmv$qsmk==0, ]$wt82_71 w0=nhefs.nmv[nhefs.nmv$qsmk==0, ]$sw ate=sum(y1*w1)/sum(w1)-sum(y0*w0)/sum(w0) ate ## [1] 3.44 Marginal structural models Estimating stabilized weights Approaches in the book: we need to estimate the stabilized weights \\(SW^A=f(A)/f(A|L)\\). For a continuous treatment \\(A\\), \\(f(A|L)\\) is a probability density function. We assume that the density \\(f(A|L)\\) is normal with mean \\(\\mu_L=E(A|L)\\) and constant variance \\(\\sigma^2\\). Then we use a linear regression model to estimate the mean \\(E(A|L)\\) and variance of residuals \\(\\sigma^2\\) for all combinations of values of \\(L\\). We also assume the density \\(f(A)\\) is normal. ## analysis restricted to subjects reporting &lt;=25 cig/day at baseline nhefs.nmv.s=subset(nhefs.nmv, smokeintensity&lt;=25) ## estimating denominator in ip weights den.fit.obj=lm( smkintensity82_71~sex+race+age+I(age^2)+ education+smokeintensity+I(smokeintensity^2)+ smokeyrs+I(smokeyrs^2)+exercise+active+wt71+I(wt71^2), data=nhefs.nmv.s ) p.den=predict(den.fit.obj, type=&quot;response&quot;) dens.den=dnorm(nhefs.nmv.s$smkintensity82_71, mean=p.den, sd=summary(den.fit.obj)$sigma) ## estimating numerator in ip weights num.fit.obj=lm(smkintensity82_71~1, data=nhefs.nmv.s) p.num=predict(num.fit.obj, type=&quot;response&quot;) dens.num=dnorm(nhefs.nmv.s$smkintensity82_71, p.num, summary(num.fit.obj)$sigma) nhefs.nmv.s$sw.a=dens.num/dens.den summary(nhefs.nmv.s$sw.a) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.19 0.89 0.97 1.00 1.05 5.10 msm.sw.cont=geeglm(wt82_71~smkintensity82_71+I(smkintensity82_71*smkintensity82_71), data=nhefs.nmv.s, weights=sw.a, id=seqn, corstr=&quot;independence&quot;) summary(msm.sw.cont) ## ## Call: ## geeglm(formula = wt82_71 ~ smkintensity82_71 + I(smkintensity82_71 * ## smkintensity82_71), data = nhefs.nmv.s, weights = sw.a, id = seqn, ## corstr = &quot;independence&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) 2.00452 0.29512 46.13 1.1e-11 *** ## smkintensity82_71 -0.10899 0.03154 11.94 0.00055 *** ## I(smkintensity82_71 * smkintensity82_71) 0.00269 0.00242 1.24 0.26489 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = independence ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 60.5 4.5 ## Number of clusters: 1162 Maximum cluster size: 1 beta=coef(msm.sw.cont) SE=coef(summary(msm.sw.cont))[,2] lcl=beta-qnorm(0.975)*SE ucl=beta+qnorm(0.975)*SE cbind(beta, lcl, ucl) ## beta lcl ucl ## (Intercept) 2.00452 1.42610 2.58295 ## smkintensity82_71 -0.10899 -0.17080 -0.04718 ## I(smkintensity82_71 * smkintensity82_71) 0.00269 -0.00204 0.00743 "]]
